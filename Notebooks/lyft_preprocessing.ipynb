{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import models\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.io import read_image\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the SDK\n",
    "import lyft_dataset_sdk\n",
    "from lyft_dataset_sdk.lyftdataset import LyftDataset, Quaternion, view_points\n",
    "from lyft_dataset_sdk.utils.geometry_utils import BoxVisibility, box_in_image, view_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 category,\n",
      "18 attribute,\n",
      "4 visibility,\n",
      "18421 instance,\n",
      "10 sensor,\n",
      "148 calibrated_sensor,\n",
      "177789 ego_pose,\n",
      "180 log,\n",
      "180 scene,\n",
      "22680 sample,\n",
      "189504 sample_data,\n",
      "638179 sample_annotation,\n",
      "1 map,\n",
      "Done loading in 8.4 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 2.9 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "data_path = Path(r\"S:\\MADS\\Capstone\\3d-object-detection-for-autonomous-vehicles\\Train\")\n",
    "json_path = r\"S:\\MADS\\Capstone\\3d-object-detection-for-autonomous-vehicles\\Train\\data\"\n",
    "lyftdata = LyftDataset(data_path=data_path, json_path=json_path, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because Lyft does not provide ground truth lables for their testing data in their Kaggle competition, we are going to treat the Validation data (taken from the training images and bounding boxes) as our testing data to perform inference on. We randomly split our training and validation data by the date they were recorded. This ensures we control for seasonality of the data and at a minimum ensures that we don't have imagery captures from the same route on the same day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_dates = sorted(list({d[\"date_captured\"] for d in lyftdata.log}))\n",
    "\n",
    "train_dates = sorted([unique_dates[i] for i in range(len(unique_dates)) if i % 2 == 0])\n",
    "validation_dates = sorted([unique_dates[i] for i in range(len(unique_dates)) if i % 2 != 0])\n",
    "\n",
    "train_logs_tokens = list({d[\"token\"] for d in lyftdata.log if d[\"date_captured\"] in train_dates})\n",
    "validation_logs_tokens = list({d[\"token\"] for d in lyftdata.log if d[\"date_captured\"] in validation_dates})\n",
    "\n",
    "train_validation_log_tokens = {\"train_logs_tokens\": train_logs_tokens, \"validation_logs_tokens\": validation_logs_tokens}\n",
    "\n",
    "# train_scenes_tokens = list({d[\"token\"] for d in lyftdata.scene if d[\"log_token\"] in train_logs_tokens})\n",
    "# validation_scenes_tokens = list({d[\"token\"] for d in lyftdata.scene if d[\"log_token\"] in validation_logs_tokens})\n",
    "\n",
    "# train_sample_tokens = list({d[\"token\"] for d in lyftdata.sample if d[\"scene_token\"] in train_scenes_tokens})\n",
    "# validation_sample_tokens = list({d[\"token\"] for d in lyftdata.sample if d[\"scene_token\"] in validation_scenes_tokens})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Utility functions to help us get our bounding boxes and corresponding images for our train set and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to find the image files that contain our pedestrian_anns, we must do the following:\n",
    "#Accumulate the superset of Samples that contain all of our pedestrian_annotations\n",
    "def image_category_train_val_split(category_name, lyft_dataset_object, train_validation_log_tokens, image_box_lookup = None, box_visibility = BoxVisibility.ALL):\n",
    "    \"\"\"\n",
    "    There wasn't a super clean way in the API to select image files that contain annotations from x class. I adapted the method they use in their .render_annotation() method.\n",
    "\n",
    "    Args:\n",
    "    train_validation_log_tokens: We expect this to be a dictionary where the two keys correspond to the training and validation logs respectively. \n",
    "    image_box_lookup: While we are iterating over the file path of an image and it's corresponding bounding box coordinates (found in the sample_data table), write these out to a dictionary to use later for IoU object detection comparison.\n",
    "        The argument specified should be a path specifying the file location to write this dictionary to. \n",
    "\n",
    "    Returns two lists: The first is a list of tuples of (file_path, Box object, camera_intrinsic, sample_token, cam) with respect to the training data. The second is a list of the same tuple structure wrt validation data.\n",
    "    \"\"\"\n",
    "    if image_box_lookup:\n",
    "        img_bx_lookup = {}\n",
    "\n",
    "    train_val_images_boxes = []\n",
    "\n",
    "    for train_val_key, split in train_validation_log_tokens.items():\n",
    "\n",
    "        scenes_tokens = list({d[\"token\"] for d in lyft_dataset_object.scene if d[\"log_token\"] in split})\n",
    "        sample_tokens = list({d[\"token\"] for d in lyft_dataset_object.sample if d[\"scene_token\"] in scenes_tokens})\n",
    "\n",
    "        \n",
    "        annos_class = lyft_dataset_object.sample_annotation\n",
    "        #Filter all annotations down to only annotations our our category of interest.\n",
    "        category_anns = [d for d in annos_class if d[\"category_name\"] == category_name]\n",
    "        #Filter further to only annotations who belong to our set.\n",
    "        split_anns = [d for d in category_anns if d[\"sample_token\"] in sample_tokens]\n",
    "\n",
    "        sample_category_tokens = set()\n",
    "        for ann in split_anns:\n",
    "            sample_category_tokens.add(ann[\"sample_token\"])\n",
    "\n",
    "        category_images_boxes = []\n",
    "        for sample_token in sample_category_tokens:\n",
    "            sample_record = lyft_dataset_object.get(\"sample\", sample_token)\n",
    "            sample_anns = sample_record[\"anns\"]\n",
    "            cams = [key for key in sample_record[\"data\"].keys() if \"CAM\" in key]\n",
    "            \n",
    "            #We unfortunately do have to iterate through each annotation that exists within the sample_record. There typically aren't more than 20-30 or so.\n",
    "            #This is because we are in search of the corresponding filepath (and image name) and these must be fetched through .get_sample_data() with the ann supplied as an argument.\n",
    "            for ann in sample_anns:\n",
    "                # Figure out which camera the object is fully visible in (this may return nothing)\n",
    "                for cam in cams:\n",
    "                    path, box, camera_intrinsic = lyft_dataset_object.get_sample_data(sample_record[\"data\"][cam], box_vis_level=box_visibility, selected_anntokens=[ann])\n",
    "                    if box:\n",
    "                        if box[0].name == category_name:\n",
    "                            #Get the 2D coordinates of our bounding box.\n",
    "                            box_coords = view_points(box[0].corners(), view = camera_intrinsic, normalize=True)[:2, :]\n",
    "                            category_images_boxes.append((str(path), box_coords, camera_intrinsic, sample_token, cam, ann))\n",
    "                            match = re.search(r'images\\\\(.+)', str(path))\n",
    "                            \n",
    "                            if match:\n",
    "                                file_name = match.group(1)\n",
    "                            if file_name in img_bx_lookup:\n",
    "                                img_bx_lookup[file_name].append(box_coords.tolist())\n",
    "                            else:\n",
    "                                img_bx_lookup[file_name] = [box_coords.tolist()]\n",
    "\n",
    "        train_val_images_boxes.append(category_images_boxes)\n",
    "        \n",
    "        # Write our image_box_lookup dict to disk.\n",
    "        if image_box_lookup:\n",
    "            out_path = os.path.join(image_box_lookup, train_val_key + f\"_{category_name}.json\")\n",
    "            os.makedirs(os.path.dirname(out_path), exist_ok=True)\n",
    "            # Write to a JSON file\n",
    "            with open(out_path, 'w') as file:\n",
    "                json.dump(img_bx_lookup, file)\n",
    "\n",
    "    \n",
    "    return train_val_images_boxes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories = [lyftdata.category[i][\"name\"] for i in range(len(lyftdata.category))]\n",
    "# image_box_lookup = r\"S:\\MADS\\Capstone\\3d-object-detection-for-autonomous-vehicles\\Train\\img_bx_lookup\"\n",
    "# for cat in categories:\n",
    "\n",
    "#     #We can skip car because we know we aren't doing this one, plus its by far the largest and most timely. Further, it would create a class imbalance.\n",
    "#     if cat != \"car\":\n",
    "#         #Obtain all of the annotations and their corresponding data via image_class_selector\n",
    "#         cat_train_val_images_boxes = image_category_train_val_split(cat, lyftdata, train_validation_log_tokens, image_box_lookup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_to_boundingbox(image_category_train_val_split, output_path, box_area_threshold = 5000):\n",
    "    path, box_coords, camera_intrinsic, sample_token, cam, ann = image_category_train_val_split\n",
    "    \n",
    "    # Calculate bounding box coordinates\n",
    "    x_min = np.min(box_coords[0])\n",
    "    y_min = np.min(box_coords[1])\n",
    "    x_max = np.max(box_coords[0])\n",
    "    y_max = np.max(box_coords[1])\n",
    "\n",
    "    # Calculate the area of the bounding box\n",
    "    box_area = (x_max - x_min) * (y_max - y_min)\n",
    "\n",
    "    if box_area > box_area_threshold:\n",
    "        image = Image.open(path)\n",
    "        match = re.search(r'images\\\\(.+)', path)\n",
    "\n",
    "        if match:\n",
    "            file_name = match.group(1)\n",
    "        else:\n",
    "            print(\"Pattern not found in the path\", path)\n",
    "            pass\n",
    "\n",
    "        # Crop the image. This is our initial cropping from the full size image. It is with respect to the bounding box size using box_area_threshold.\n",
    "        # Pillow uses a system of (left, upper, right, lower)\n",
    "        image_cropped = image.crop((x_min, y_min, x_max, y_max))\n",
    "\n",
    "        # Check if the directory exists, if not, create it\n",
    "        Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "        #Save the image\n",
    "        image_cropped.save(os.path.join(output_path, \"cropped_\" + file_name[:-5] + \"_\" + ann + file_name[-5:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# categories = [lyftdata.category[i][\"name\"] for i in range(len(lyftdata.category))]\n",
    "# for cat in categories:\n",
    "\n",
    "#     #We can skip car because we know we aren't doing this one, plus its by far the largest and most timely.\n",
    "#     if cat != \"car\":\n",
    "#         #Obtain all of the annotations and their corresponding data via image_class_selector\n",
    "#         cat_train_val_images_boxes = image_category_train_val_split(cat, lyftdata, train_validation_log_tokens= [train_logs_tokens, validation_logs_tokens])\n",
    "\n",
    "#         cat_train = cat_train_val_images_boxes[0]\n",
    "#         cat_val = cat_train_val_images_boxes[1]\n",
    "\n",
    "#         #Do the actual cropping and write cropped images to disk\n",
    "#         train_image_output_path = os.path.join(r\"S:\\MADS\\Capstone\\3d-object-detection-for-autonomous-vehicles\\Train\\images\\train_cropped_images\", cat +\"_cropped\")\n",
    "#         val_image_output_path = os.path.join(r\"S:\\MADS\\Capstone\\3d-object-detection-for-autonomous-vehicles\\Train\\images\\validation_cropped_images\", cat +\"_cropped\")\n",
    "        \n",
    "#         for i in range(len(cat_train)):\n",
    "#             crop_to_boundingbox(cat_train[i], output_path=train_image_output_path, box_area_threshold=5000)\n",
    "\n",
    "#         for i in range(len(cat_val)):\n",
    "#             crop_to_boundingbox(cat_val[i], output_path=val_image_output_path, box_area_threshold=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess pedestrain images with additional processes like Padding and Center cropping for input into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'str' object cannot be interpreted as an integer",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32ms:\\MADS\\Capstone\\3d-object-detection-for-autonomous-vehicles\\Notebooks\\lyft_preprocessing.ipynb Cell 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/s%3A/MADS/Capstone/3d-object-detection-for-autonomous-vehicles/Notebooks/lyft_preprocessing.ipynb#X21sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m [d[\u001b[39m\"\u001b[39;49m\u001b[39mname\u001b[39;49m\u001b[39m\"\u001b[39;49m] \u001b[39mfor\u001b[39;49;00m d \u001b[39min\u001b[39;49;00m lyftdata\u001b[39m.\u001b[39;49mcategory]\u001b[39m.\u001b[39;49mpop(\u001b[39m\"\u001b[39;49m\u001b[39mcar\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "\u001b[1;31mTypeError\u001b[0m: 'str' object cannot be interpreted as an integer"
     ]
    }
   ],
   "source": [
    "[d[\"name\"] for d in lyftdata.category if d[\"name\"] != \"car\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_transform(cropped_images_dir, image_sizes_list, lyftdata, partial_images = False):\n",
    "    \"\"\"\n",
    "    partial_images: If set to true, create additional \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "    for size in image_sizes_list:\n",
    "        categories = [d[\"name\"] for d in lyftdata.category if d[\"name\"] != \"car\"]\n",
    "        for category in categories:\n",
    "            category_dir = os.path.join(cropped_images_dir, category)\n",
    "            if partial_images:\n",
    "                new_out_dir = os.path.join(cropped_images_dir, category + \"_\" + str(size) + \"_partials\")\n",
    "            else:\n",
    "                new_out_dir = os.path.join(cropped_images_dir, category + \"_\" + str(size))\n",
    "                \n",
    "            if not os.path.exists(new_out_dir):\n",
    "                os.makedirs(new_out_dir)\n",
    "\n",
    "            #For each image in the directory \"category\", modify it such that it is resized to \"size\".\n",
    "            for image_file in os.listdir(category_dir):\n",
    "                image_path = os.path.join(category_dir, image_file)\n",
    "                image_main = Image.open(image_path)\n",
    "                image_main_width, image_main_height = image_main.size\n",
    "                images = [image_main]\n",
    "\n",
    "                if partial_images:\n",
    "                    image_left = image_main.crop((0, 0, image_main_width // 2, image_main_height))\n",
    "                    image_right = image_main.crop((0, 0, image_main_width // 2, image_main_height))\n",
    "                    images.extend([image_left, image_right])\n",
    "\n",
    "                for image in images:\n",
    "                    image_width, image_height = image.size\n",
    "\n",
    "                    if image_width < size or image_height < size:\n",
    "                        # Calculate padding\n",
    "                        padding_left = (size - image_width) // 2 if image_width < size else 0\n",
    "                        padding_top = (size - image_height) // 2 if image_height < size else 0\n",
    "\n",
    "                        # Adjust for odd total padding\n",
    "                        padding_right = size - image_width - padding_left\n",
    "                        padding_bottom = size - image_height - padding_top\n",
    "\n",
    "                        # Apply padding\n",
    "                        image = ImageOps.expand(image, (padding_left, padding_top, padding_right, padding_bottom), fill=0)\n",
    "\n",
    "                    elif image_width > size or image_height > size:\n",
    "                        # Crop the image to the required size if it's larger\n",
    "                        image = transforms.CenterCrop(size)(image)\n",
    "\n",
    "                    # No else case needed, as no action is taken if the image is already the required size\n",
    "                    if image.size != (size, size):\n",
    "                        # Raise an exception\n",
    "                        raise RuntimeError(f\"Image size after transformation does not match required size of {size} for image and instead is of size {image.size}: {image_path}. Original image size: {image_width} {image_height}\")\n",
    "                    \n",
    "                    # Save the transformed image\n",
    "                    image.save(os.path.join(new_out_dir, image_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only run the code in the cell below if we haven't yet preprocessed any images or if we need to process them at a new, different size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cropped_images_dir = r\"S:\\MADS\\Capstone\\3d-object-detection-for-autonomous-vehicles\\Train\\images\\train_cropped_images\"\n",
    "# sizes = [224]\n",
    "# dynamic_transform(cropped_images_dir, sizes, lyftdata, partial_images=True)\n",
    "\n",
    "# print(\"transforms_with_partials done.\")\n",
    "\n",
    "# dynamic_transform(cropped_images_dir, sizes, lyftdata, partial_images=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
