{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import models\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.io import read_image\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the SDK\n",
    "import lyft_dataset_sdk\n",
    "from lyft_dataset_sdk.lyftdataset import LyftDataset, LyftDatasetExplorer, Quaternion, view_points\n",
    "from lyft_dataset_sdk.utils.data_classes import LidarPointCloud\n",
    "from lyft_dataset_sdk.utils.geometry_utils import BoxVisibility, box_in_image, view_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 category,\n",
      "18 attribute,\n",
      "4 visibility,\n",
      "18421 instance,\n",
      "10 sensor,\n",
      "148 calibrated_sensor,\n",
      "177789 ego_pose,\n",
      "180 log,\n",
      "180 scene,\n",
      "22680 sample,\n",
      "189504 sample_data,\n",
      "638179 sample_annotation,\n",
      "1 map,\n",
      "Done loading in 9.8 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 2.6 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "data_path = Path(r\"S:\\MADS\\Capstone\\3d-object-detection-for-autonomous-vehicles\\Train\")\n",
    "json_path = r\"S:\\MADS\\Capstone\\3d-object-detection-for-autonomous-vehicles\\Train\\data\"\n",
    "lyftdata = LyftDataset(data_path=data_path, json_path=json_path, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Selection/Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['2019-01-02', '2019-01-04', '2019-01-07', '2019-01-09', '2019-01-11', '2019-01-24', '2019-01-25', '2019-01-28', '2019-01-29', '2019-01-30', '2019-01-31', '2019-02-01', '2019-02-05', '2019-02-06', '2019-02-07', '2019-02-08', '2019-02-11', '2019-02-12', '2019-02-13', '2019-02-17', '2019-02-18', '2019-02-19', '2019-02-20', '2019-02-28', '2019-03-04', '2019-03-06', '2019-03-07', '2019-03-08', '2019-03-22', '2019-03-25', '2019-05-01', '2019-05-02', '2019-05-03', '2019-05-06', '2019-05-09', '2019-05-10', '2019-05-14', '2019-05-17', '2019-05-21', '2019-05-22', '2019-05-23', '2019-05-24', '2019-05-28']\n",
      "['2019-01-02', '2019-01-07', '2019-01-11', '2019-01-25', '2019-01-29', '2019-01-31', '2019-02-05', '2019-02-07', '2019-02-11', '2019-02-13', '2019-02-18', '2019-02-20', '2019-03-04', '2019-03-07', '2019-03-22', '2019-05-01', '2019-05-03', '2019-05-09', '2019-05-14', '2019-05-21', '2019-05-23', '2019-05-28']\n",
      "['2019-01-04', '2019-01-09', '2019-01-24', '2019-01-28', '2019-01-30', '2019-02-01', '2019-02-06', '2019-02-08', '2019-02-12', '2019-02-17', '2019-02-19', '2019-02-28', '2019-03-06', '2019-03-08', '2019-03-25', '2019-05-02', '2019-05-06', '2019-05-10', '2019-05-17', '2019-05-22', '2019-05-24']\n"
     ]
    }
   ],
   "source": [
    "unique_dates = sorted(list({d[\"date_captured\"] for d in lyftdata.log}))\n",
    "\n",
    "train_dates = sorted([unique_dates[i] for i in range(len(unique_dates)) if i % 2 == 0])\n",
    "validation_dates = sorted([unique_dates[i] for i in range(len(unique_dates)) if i % 2 != 0])\n",
    "print(sorted([date for date in unique_dates]))\n",
    "print(train_dates)\n",
    "print(validation_dates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 96\n"
     ]
    }
   ],
   "source": [
    "train_logs_tokens = list({d[\"token\"] for d in lyftdata.log if d[\"date_captured\"] in train_dates})\n",
    "validation_logs_tokens = list({d[\"token\"] for d in lyftdata.log if d[\"date_captured\"] in validation_dates})\n",
    "print(len(train_logs_tokens), len(validation_logs_tokens))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 96\n"
     ]
    }
   ],
   "source": [
    "train_scenes_tokens = list({d[\"token\"] for d in lyftdata.scene if d[\"log_token\"] in train_logs_tokens})\n",
    "validation_scenes_tokens = list({d[\"token\"] for d in lyftdata.scene if d[\"log_token\"] in validation_logs_tokens})\n",
    "print(len(train_scenes_tokens), len(validation_scenes_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10584 12096\n"
     ]
    }
   ],
   "source": [
    "train_sample_tokens = list({d[\"token\"] for d in lyftdata.sample if d[\"scene_token\"] in train_logs_tokens})\n",
    "validation_sample_tokens = list({d[\"token\"] for d in lyftdata.sample if d[\"scene_token\"] in validation_logs_tokens})\n",
    "print(len(train_sample_tokens), len(validation_sample_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Utility functions to help us get our bounding boxes and corresponding images for our train set and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to find the image files that contain our pedestrian_anns, we must do the following:\n",
    "#Accumulate the superset of Samples that contain all of our pedestrian_annotations\n",
    "def img_class_selector(class_name, lyft_dataset_object, train_validation_split = None, box_visibility = BoxVisibility.ALL):\n",
    "    \"\"\"\n",
    "    There wasn't a super clean way in the API to select image files that contain annotations from x class. I adapted the method they use in their .render_annotation() method.\n",
    "\n",
    "    Returns tuples of (file_path, Box object, camera_intrinsic, sample_token, cam)\n",
    "    \"\"\"\n",
    "\n",
    "    annos_class = lyft_dataset_object.sample_annotation\n",
    "    #Filter all annotations down to only annotations our our class of interest.\n",
    "    class_anns = [d for d in annos_class if d[\"category_name\"] == class_name]\n",
    "    sample_class_tokens = set()\n",
    "    for ann in class_anns:\n",
    "        sample_class_tokens.add(ann[\"sample_token\"])\n",
    "\n",
    "    class_images_boxes = []\n",
    "    for sample_token in sample_class_tokens:\n",
    "        sample_record = lyft_dataset_object.get(\"sample\", sample_token)\n",
    "        sample_anns = sample_record[\"anns\"]\n",
    "        cams = [key for key in sample_record[\"data\"].keys() if \"CAM\" in key]\n",
    "        \n",
    "        #We unfortunately do have to iterate through each annotation that exists within the sample_record. There typically aren't more than 20-30 or so.\n",
    "        for ann in sample_anns:\n",
    "            # Figure out which camera the object is fully visible in (this may return nothing)\n",
    "            for cam in cams:\n",
    "                path, box, camera_intrinsic = lyft_dataset_object.get_sample_data(sample_record[\"data\"][cam], box_vis_level=box_visibility, selected_anntokens=[ann])\n",
    "                if box:\n",
    "                    if box[0].name == class_name:\n",
    "                        class_images_boxes.append((str(path), box[0], camera_intrinsic, sample_token, cam, ann))\n",
    "\n",
    "    return class_images_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_select_images(img_class_selector_output, output_path, output_size ,box_area_threshold = 5000):\n",
    "    path, box, camera_intrinsic, sample_token, cam, ann = img_class_selector_output\n",
    "    \n",
    "    box_coords = view_points(box.corners(), view = camera_intrinsic, normalize=True)[:2, :]\n",
    "    \n",
    "    # Calculate bounding box coordinates\n",
    "    x_min = np.min(box_coords[0])\n",
    "    y_min = np.min(box_coords[1])\n",
    "    x_max = np.max(box_coords[0])\n",
    "    y_max = np.max(box_coords[1])\n",
    "\n",
    "    # Calculate the area of the bounding box\n",
    "    box_area = (x_max - x_min) * (y_max - y_min)\n",
    "\n",
    "    if box_area > box_area_threshold:\n",
    "        img = Image.open(path)\n",
    "        match = re.search(r'images\\\\(.+)', path)\n",
    "\n",
    "        if match:\n",
    "            file_name = match.group(1)\n",
    "        else:\n",
    "            print(\"Pattern not found in the path\", path)\n",
    "            pass\n",
    "\n",
    "        # Crop the image. This is our initial cropping from the full size image. It is with respect to the bounding box size using box_area_threshold.\n",
    "        # Pillow uses a system of (left, upper, right, lower)\n",
    "        img_cropped = img.crop((x_min, y_min, x_max, y_max))\n",
    "\n",
    "        # Check if the directory exists, if not, create it\n",
    "        Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "        #Save the image\n",
    "        img_cropped.save(os.path.join(output_path, \"cropped_\" + file_name[:-5] + \"_\" + ann + file_name[-5:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car',\n",
       " 'pedestrian',\n",
       " 'animal',\n",
       " 'other_vehicle',\n",
       " 'bus',\n",
       " 'motorcycle',\n",
       " 'truck',\n",
       " 'emergency_vehicle',\n",
       " 'bicycle']"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = [lyftdata.category[i][\"name\"] for i in range(len(lyftdata.category))]\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ped_images_boxes = img_class_selector(\"pedestrian\")\n",
    "# crop_select_images(ped_images_boxes)\n",
    "\n",
    "# categories = [lyftdata.category[i][\"name\"] for i in range(len(lyftdata.category))]\n",
    "# for cat in categories:\n",
    "\n",
    "#     #We can skip car because we know we aren't doing this one, plus its by far the largest and most timely.\n",
    "#     if cat != \"car\":\n",
    "#         #Obtain all of the annotations and their corresponding data via image_class_selector\n",
    "#         cat_images_boxes = img_class_selector(cat, lyftdata)\n",
    "\n",
    "#         #Do the actual cropping and write cropped images to disk\n",
    "#         img_output_path = os.path.join(r\"S:\\MADS\\Capstone\\3d-object-detection-for-autonomous-vehicles\\images\", cat +\"_cropped\")\n",
    "        \n",
    "#         for i in range(len(cat_images_boxes)):\n",
    "#             crop_select_images(cat_images_boxes[i], output_path=img_output_path, box_area_threshold=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess pedestrain images with additional processes like Padding and Center cropping for input into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dynamic_transform(cropped_images_dir, image_sizes_list):\n",
    "\n",
    "    for size in image_sizes_list:\n",
    "        categories = os.listdir(cropped_images_dir)\n",
    "        for category in categories:\n",
    "            category_dir = os.path.join(cropped_images_dir, category)\n",
    "            new_out_dir = os.path.join(cropped_images_dir, category + \"_\" + str(size))\n",
    "            if not os.path.exists(new_out_dir):\n",
    "                os.makedirs(new_out_dir)\n",
    "\n",
    "            #For each image in the directory \"category\", modify it such that it is resized to \"size\".\n",
    "            for image_file in os.listdir(category_dir):\n",
    "                img_path = os.path.join(category_dir, image_file)\n",
    "                img = Image.open(img_path)\n",
    "                img_width, img_height = img.size\n",
    "\n",
    "                if img_width < size or img_height < size:\n",
    "                    # Calculate padding\n",
    "                    padding_left = (size - img_width) // 2 if img_width < size else 0\n",
    "                    padding_top = (size - img_height) // 2 if img_height < size else 0\n",
    "\n",
    "                    # Adjust for odd total padding\n",
    "                    padding_right = size - img_width - padding_left\n",
    "                    padding_bottom = size - img_height - padding_top\n",
    "\n",
    "                    # Apply padding\n",
    "                    img = ImageOps.expand(img, (padding_left, padding_top, padding_right, padding_bottom), fill=0)\n",
    "\n",
    "                elif img_width > size or img_height > size:\n",
    "                    # Crop the image to the required size if it's larger\n",
    "                    img = transforms.CenterCrop(size)(img)\n",
    "\n",
    "                # No else case needed, as no action is taken if the image is already the required size\n",
    "                if img.size != (size, size):\n",
    "                    #Throw an exception here and break the method. Before breaking, add img_path to self.problem_images\n",
    "                        # Raise an exception\n",
    "                    raise RuntimeError(f\"Image size after transformation does not match required size of {size} for image and instead is of size {img.size}: {img_path}. Original image size: {img_width} {img_height}\")\n",
    "                \n",
    "                # Save the transformed image\n",
    "                img.save(os.path.join(new_out_dir, image_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Only run the code in the cell below if we haven't yet preprocessed any images or if we need to process them at a new, different size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cropped_images_dir = r\"S:\\MADS\\Capstone\\3d-object-detection-for-autonomous-vehicles\\images\\cropped_images\"\n",
    "# sizes = [224]\n",
    "# dynamic_transform(cropped_images_dir, sizes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define our custom Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lyft_experimental_CustomDataset(Dataset):\n",
    "    def __init__(self, cropped_images_dir, target_class_name, required_size=224):\n",
    "        self.cropped_images_dir = cropped_images_dir\n",
    "        self.required_size = required_size\n",
    "        self.image_filenames = os.listdir(cropped_images_dir)\n",
    "        self.problem_images = []\n",
    "        self.data = []\n",
    "        #Our class will ingest cropped images from all classes. We must add corresponding labels to these images.\n",
    "        #The images are stored in directories wrt their class. So, we use the dir name to label the image.\n",
    "        # Iterate over each subdirectory in the main directory\n",
    "        for class_name in os.listdir(cropped_images_dir):\n",
    "            class_dir = os.path.join(cropped_images_dir, class_name)\n",
    "            if os.path.isdir(class_dir):  # Check if it's a directory\n",
    "                is_target_class = class_name == target_class_name\n",
    "                label = 1 if is_target_class else 0\n",
    "\n",
    "                for filename in os.listdir(class_dir):\n",
    "                    file_path = os.path.join(class_dir, filename)\n",
    "                    if os.path.isfile(file_path):  # Check if it's a file\n",
    "                        self.data.append((file_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        # Convert the image to tensor and normalize\n",
    "        image = transforms.ToTensor()(image)\n",
    "        image = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'S:\\\\MADS\\\\Capstone\\\\3d-object-detection-for-autonomous-vehicles\\\\images\\\\cropped_images\\\\224'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32ms:\\MADS\\Capstone\\3d-object-detection-for-autonomous-vehicles\\Notebooks\\sterny_boxes.ipynb Cell 14\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/s%3A/MADS/Capstone/3d-object-detection-for-autonomous-vehicles/Notebooks/sterny_boxes.ipynb#X16sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m cropped_images_dir \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m'\u001b[39m\u001b[39mS:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mMADS\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mCapstone\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m3d-object-detection-for-autonomous-vehicles\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mimages\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mcropped_images\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m224\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/s%3A/MADS/Capstone/3d-object-detection-for-autonomous-vehicles/Notebooks/sterny_boxes.ipynb#X16sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m dataset \u001b[39m=\u001b[39m Lyft_experimental_CustomDataset(cropped_images_dir, target_class_name \u001b[39m=\u001b[39;49m \u001b[39m\"\u001b[39;49m\u001b[39mpedestrian\u001b[39;49m\u001b[39m\"\u001b[39;49m, required_size\u001b[39m=\u001b[39;49m\u001b[39m224\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/s%3A/MADS/Capstone/3d-object-detection-for-autonomous-vehicles/Notebooks/sterny_boxes.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m dataloader \u001b[39m=\u001b[39m DataLoader(dataset, batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m, shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[1;32ms:\\MADS\\Capstone\\3d-object-detection-for-autonomous-vehicles\\Notebooks\\sterny_boxes.ipynb Cell 14\u001b[0m line \u001b[0;36m5\n\u001b[0;32m      <a href='vscode-notebook-cell:/s%3A/MADS/Capstone/3d-object-detection-for-autonomous-vehicles/Notebooks/sterny_boxes.ipynb#X16sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcropped_images_dir \u001b[39m=\u001b[39m cropped_images_dir\n\u001b[0;32m      <a href='vscode-notebook-cell:/s%3A/MADS/Capstone/3d-object-detection-for-autonomous-vehicles/Notebooks/sterny_boxes.ipynb#X16sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequired_size \u001b[39m=\u001b[39m required_size\n\u001b[1;32m----> <a href='vscode-notebook-cell:/s%3A/MADS/Capstone/3d-object-detection-for-autonomous-vehicles/Notebooks/sterny_boxes.ipynb#X16sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mimage_filenames \u001b[39m=\u001b[39m os\u001b[39m.\u001b[39;49mlistdir(cropped_images_dir)\n\u001b[0;32m      <a href='vscode-notebook-cell:/s%3A/MADS/Capstone/3d-object-detection-for-autonomous-vehicles/Notebooks/sterny_boxes.ipynb#X16sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mproblem_images \u001b[39m=\u001b[39m []\n\u001b[0;32m      <a href='vscode-notebook-cell:/s%3A/MADS/Capstone/3d-object-detection-for-autonomous-vehicles/Notebooks/sterny_boxes.ipynb#X16sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m []\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'S:\\\\MADS\\\\Capstone\\\\3d-object-detection-for-autonomous-vehicles\\\\images\\\\cropped_images\\\\224'"
     ]
    }
   ],
   "source": [
    "cropped_images_dir = r'S:\\MADS\\Capstone\\3d-object-detection-for-autonomous-vehicles\\images\\cropped_images\\224'\n",
    "dataset = Lyft_experimental_CustomDataset(cropped_images_dir, target_class_name = \"pedestrian\", required_size=224)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
      "\n",
      "\n",
      "        [[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
      "\n",
      "\n",
      "        [[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
      "\n",
      "\n",
      "        [[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
      "\n",
      "\n",
      "        [[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "for data in dataloader:\n",
    "    print(data[0])\n",
    "    print(data[1])\n",
    "    # print(data[1][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Labels - tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0])\n",
      "Batch 1: Labels - tensor([0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 1])\n",
      "Batch 2: Labels - tensor([0, 0, 1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 1, 0, 0, 0, 1,\n",
      "        1, 0, 0, 1, 1, 0, 1, 0])\n",
      "Batch 3: Labels - tensor([0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0,\n",
      "        0, 0, 0, 0, 1, 0, 0, 0])\n",
      "Batch 4: Labels - tensor([0, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 1, 0])\n",
      "Batch 5: Labels - tensor([1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
      "        1, 0, 0, 0, 0, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(dataloader):\n",
    "    print(f\"Batch {i}: Labels - {data[1]}\")\n",
    "    if i == 5:  # Check first 5 batches\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         ...,\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "        [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         ...,\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "        [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         ...,\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]), 0)\n",
      "(tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         ...,\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "        [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         ...,\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "        [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         ...,\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]), 0)\n",
      "(tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         ...,\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "        [[-2.0357, -2.0357, -2.0357,  ..., -2.0182, -2.0182, -2.0182],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0182, -2.0182, -2.0182],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         ...,\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "        [[-1.7347, -1.7347, -1.7347,  ..., -1.7347, -1.7347, -1.7347],\n",
      "         [-1.7347, -1.7347, -1.7347,  ..., -1.7347, -1.7347, -1.7347],\n",
      "         [-1.7696, -1.7696, -1.7696,  ..., -1.7696, -1.7696, -1.7696],\n",
      "         ...,\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]), 0)\n",
      "(tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         ...,\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "        [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         ...,\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "        [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         ...,\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]), 0)\n",
      "(tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         ...,\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "        [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         ...,\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "        [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         ...,\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]), 0)\n",
      "(tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         ...,\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "        [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         ...,\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "        [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         ...,\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]), 0)\n",
      "(tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         ...,\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "        [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         ...,\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "        [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         ...,\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]), 0)\n",
      "(tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         ...,\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "        [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         ...,\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "        [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         ...,\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]), 0)\n",
      "(tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         ...,\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "        [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         ...,\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "        [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         ...,\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]), 0)\n",
      "(tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         ...,\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "        [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         ...,\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "        [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         ...,\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]), 0)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):  # Test the first 10 items\n",
    "    try:\n",
    "        print(dataset[i])\n",
    "    except Exception as e:\n",
    "        print(f\"Error at index {i}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Pytorch models and train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n"
     ]
    }
   ],
   "source": [
    "model_res50 = models.resnet50(pretrained=True)\n",
    "num_ftrs = model_res50.fc.in_features\n",
    "print(num_ftrs)\n",
    "model_res50.fc = nn.Linear(num_ftrs, 2) # Modify the last layer for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model_res50.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if GPU is available and move the model to GPU if it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model_res50 = model_res50.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train through n epochs. In each epoch, we set the model into train mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/9 Loss: 0.1191 Acc: 0.9553\n",
      "Epoch 1/9 Loss: 0.0849 Acc: 0.9677\n",
      "Epoch 2/9 Loss: 0.0723 Acc: 0.9738\n",
      "Epoch 3/9 Loss: 0.0673 Acc: 0.9755\n",
      "Epoch 4/9 Loss: 0.0566 Acc: 0.9786\n",
      "Epoch 5/9 Loss: 0.0501 Acc: 0.9812\n",
      "Epoch 6/9 Loss: 0.0440 Acc: 0.9837\n",
      "Epoch 7/9 Loss: 0.0388 Acc: 0.9862\n",
      "Epoch 8/9 Loss: 0.0356 Acc: 0.9869\n",
      "Epoch 9/9 Loss: 0.0303 Acc: 0.9891\n"
     ]
    }
   ],
   "source": [
    "# # Number of epochs to train for\n",
    "# num_epochs = 10\n",
    "\n",
    "# for epoch in range(num_epochs):\n",
    "#     #Set training mode at beginning of each epoch in case we train on validation data within epoch with requires model.eval()\n",
    "#     model_res50.train()  # Set the model to training mode\n",
    "\n",
    "#     running_loss = 0.0\n",
    "#     running_corrects = 0\n",
    "\n",
    "#     # Iterate over data\n",
    "#     for inputs, labels in dataloader:\n",
    "#         inputs = inputs.to(device)\n",
    "#         labels = labels.to(device)\n",
    "\n",
    "#         # Zero the parameter gradients\n",
    "#         optimizer.zero_grad()\n",
    "\n",
    "#         # Forward pass\n",
    "#         outputs = model_res50(inputs)\n",
    "#         loss = criterion(outputs, labels)\n",
    "\n",
    "#         # Backward pass and optimize\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         # Statistics\n",
    "#         running_loss += loss.item() * inputs.size(0)\n",
    "#         _, preds = torch.max(outputs, 1)\n",
    "#         running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "#     epoch_loss = running_loss / len(dataset)\n",
    "#     epoch_acc = running_corrects.double() / len(dataset)\n",
    "\n",
    "#     print(f'Epoch {epoch}/{num_epochs - 1} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "\n",
    "    # # Assuming you have the final loss and accuracy in variables `final_loss` and `final_acc`\n",
    "    # final_loss = 0.0303\n",
    "    # final_acc = 0.9891\n",
    "\n",
    "    # # Save the model state and metrics in a dictionary\n",
    "    # checkpoint = {\n",
    "    #     'model_state_dict': model_res50.state_dict(),\n",
    "    #     'final_loss': final_loss,\n",
    "    #     'final_accuracy': final_acc\n",
    "    # }\n",
    "\n",
    "    # # Save the checkpoint\n",
    "    # torch.save(checkpoint, 'model_res50_checkpoint.pth')\n",
    "\n",
    "    # # To load the model and metrics later\n",
    "    # checkpoint = torch.load('model_res50_checkpoint.pth')\n",
    "    # model_res50.load_state_dict(checkpoint['model_state_dict'])\n",
    "    # loaded_final_loss = checkpoint['final_loss']\n",
    "    # loaded_final_acc = checkpoint['final_accuracy']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# torch.save(model_res50, r\"S:\\MADS\\Capstone\\3d-object-detection-for-autonomous-vehicles\\models\\resnet50_2023_11_19_01_47\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the model we trained back into our Notbook after previously saving it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_res50 = torch.load(r\"S:\\MADS\\Capstone\\3d-object-detection-for-autonomous-vehicles\\Train\\models\\resnet50_2023_11_19_01_47\")\n",
    "model_res50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: conv1 | Kernel Size: (7, 7) | Output Channels: 64\n",
      "Layer: layer1.0.conv1 | Kernel Size: (1, 1) | Output Channels: 64\n",
      "Layer: layer1.0.conv2 | Kernel Size: (3, 3) | Output Channels: 64\n",
      "Layer: layer1.0.conv3 | Kernel Size: (1, 1) | Output Channels: 256\n",
      "Layer: layer1.0.downsample.0 | Kernel Size: (1, 1) | Output Channels: 256\n",
      "Layer: layer1.1.conv1 | Kernel Size: (1, 1) | Output Channels: 64\n",
      "Layer: layer1.1.conv2 | Kernel Size: (3, 3) | Output Channels: 64\n",
      "Layer: layer1.1.conv3 | Kernel Size: (1, 1) | Output Channels: 256\n",
      "Layer: layer1.2.conv1 | Kernel Size: (1, 1) | Output Channels: 64\n",
      "Layer: layer1.2.conv2 | Kernel Size: (3, 3) | Output Channels: 64\n",
      "Layer: layer1.2.conv3 | Kernel Size: (1, 1) | Output Channels: 256\n",
      "Layer: layer2.0.conv1 | Kernel Size: (1, 1) | Output Channels: 128\n",
      "Layer: layer2.0.conv2 | Kernel Size: (3, 3) | Output Channels: 128\n",
      "Layer: layer2.0.conv3 | Kernel Size: (1, 1) | Output Channels: 512\n",
      "Layer: layer2.0.downsample.0 | Kernel Size: (1, 1) | Output Channels: 512\n",
      "Layer: layer2.1.conv1 | Kernel Size: (1, 1) | Output Channels: 128\n",
      "Layer: layer2.1.conv2 | Kernel Size: (3, 3) | Output Channels: 128\n",
      "Layer: layer2.1.conv3 | Kernel Size: (1, 1) | Output Channels: 512\n",
      "Layer: layer2.2.conv1 | Kernel Size: (1, 1) | Output Channels: 128\n",
      "Layer: layer2.2.conv2 | Kernel Size: (3, 3) | Output Channels: 128\n",
      "Layer: layer2.2.conv3 | Kernel Size: (1, 1) | Output Channels: 512\n",
      "Layer: layer2.3.conv1 | Kernel Size: (1, 1) | Output Channels: 128\n",
      "Layer: layer2.3.conv2 | Kernel Size: (3, 3) | Output Channels: 128\n",
      "Layer: layer2.3.conv3 | Kernel Size: (1, 1) | Output Channels: 512\n",
      "Layer: layer3.0.conv1 | Kernel Size: (1, 1) | Output Channels: 256\n",
      "Layer: layer3.0.conv2 | Kernel Size: (3, 3) | Output Channels: 256\n",
      "Layer: layer3.0.conv3 | Kernel Size: (1, 1) | Output Channels: 1024\n",
      "Layer: layer3.0.downsample.0 | Kernel Size: (1, 1) | Output Channels: 1024\n",
      "Layer: layer3.1.conv1 | Kernel Size: (1, 1) | Output Channels: 256\n",
      "Layer: layer3.1.conv2 | Kernel Size: (3, 3) | Output Channels: 256\n",
      "Layer: layer3.1.conv3 | Kernel Size: (1, 1) | Output Channels: 1024\n",
      "Layer: layer3.2.conv1 | Kernel Size: (1, 1) | Output Channels: 256\n",
      "Layer: layer3.2.conv2 | Kernel Size: (3, 3) | Output Channels: 256\n",
      "Layer: layer3.2.conv3 | Kernel Size: (1, 1) | Output Channels: 1024\n",
      "Layer: layer3.3.conv1 | Kernel Size: (1, 1) | Output Channels: 256\n",
      "Layer: layer3.3.conv2 | Kernel Size: (3, 3) | Output Channels: 256\n",
      "Layer: layer3.3.conv3 | Kernel Size: (1, 1) | Output Channels: 1024\n",
      "Layer: layer3.4.conv1 | Kernel Size: (1, 1) | Output Channels: 256\n",
      "Layer: layer3.4.conv2 | Kernel Size: (3, 3) | Output Channels: 256\n",
      "Layer: layer3.4.conv3 | Kernel Size: (1, 1) | Output Channels: 1024\n",
      "Layer: layer3.5.conv1 | Kernel Size: (1, 1) | Output Channels: 256\n",
      "Layer: layer3.5.conv2 | Kernel Size: (3, 3) | Output Channels: 256\n",
      "Layer: layer3.5.conv3 | Kernel Size: (1, 1) | Output Channels: 1024\n",
      "Layer: layer4.0.conv1 | Kernel Size: (1, 1) | Output Channels: 512\n",
      "Layer: layer4.0.conv2 | Kernel Size: (3, 3) | Output Channels: 512\n",
      "Layer: layer4.0.conv3 | Kernel Size: (1, 1) | Output Channels: 2048\n",
      "Layer: layer4.0.downsample.0 | Kernel Size: (1, 1) | Output Channels: 2048\n",
      "Layer: layer4.1.conv1 | Kernel Size: (1, 1) | Output Channels: 512\n",
      "Layer: layer4.1.conv2 | Kernel Size: (3, 3) | Output Channels: 512\n",
      "Layer: layer4.1.conv3 | Kernel Size: (1, 1) | Output Channels: 2048\n",
      "Layer: layer4.2.conv1 | Kernel Size: (1, 1) | Output Channels: 512\n",
      "Layer: layer4.2.conv2 | Kernel Size: (3, 3) | Output Channels: 512\n",
      "Layer: layer4.2.conv3 | Kernel Size: (1, 1) | Output Channels: 2048\n"
     ]
    }
   ],
   "source": [
    "for name, layer in model_res50.named_modules():\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        print(f\"Layer: {name} | Kernel Size: {layer.kernel_size} | Output Channels: {layer.out_channels}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize the output of the first layer which is a convolution layer with kernel 7x7. This is just to remind us what we are dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5216, 0.5564, 0.5353, 0.5167, 0.4716, 0.4680, 0.4344],\n",
      "         [0.5082, 0.5583, 0.5599, 0.5863, 0.5748, 0.5894, 0.5448],\n",
      "         [0.4710, 0.5067, 0.4942, 0.5590, 0.6124, 0.6288, 0.5865],\n",
      "         [0.4613, 0.4849, 0.4238, 0.4466, 0.4884, 0.5480, 0.5525],\n",
      "         [0.4531, 0.5083, 0.4590, 0.4340, 0.3869, 0.4033, 0.4516],\n",
      "         [0.4771, 0.5497, 0.5646, 0.5864, 0.5306, 0.4522, 0.4601],\n",
      "         [0.4304, 0.5027, 0.5219, 0.5721, 0.5580, 0.5262, 0.4961]],\n",
      "\n",
      "        [[0.4899, 0.5471, 0.5639, 0.5793, 0.5490, 0.5380, 0.4868],\n",
      "         [0.4820, 0.5518, 0.5897, 0.6602, 0.6837, 0.6995, 0.6458],\n",
      "         [0.4087, 0.4283, 0.4232, 0.5226, 0.6307, 0.6961, 0.6625],\n",
      "         [0.4011, 0.3911, 0.2941, 0.2801, 0.3454, 0.4737, 0.5306],\n",
      "         [0.4518, 0.4842, 0.4118, 0.3183, 0.2340, 0.2535, 0.3518],\n",
      "         [0.5183, 0.6014, 0.6266, 0.6254, 0.5223, 0.3980, 0.3885],\n",
      "         [0.4621, 0.5618, 0.6192, 0.6895, 0.6572, 0.5823, 0.5073]],\n",
      "\n",
      "        [[0.4855, 0.5223, 0.5318, 0.5455, 0.5168, 0.5174, 0.4859],\n",
      "         [0.4722, 0.5169, 0.5451, 0.6123, 0.6245, 0.6454, 0.6135],\n",
      "         [0.3955, 0.3981, 0.3911, 0.4965, 0.5864, 0.6434, 0.6221],\n",
      "         [0.4139, 0.3902, 0.2937, 0.2976, 0.3515, 0.4620, 0.5142],\n",
      "         [0.4658, 0.4808, 0.4056, 0.3378, 0.2605, 0.2780, 0.3591],\n",
      "         [0.5383, 0.5957, 0.6098, 0.6112, 0.5151, 0.3955, 0.3915],\n",
      "         [0.5014, 0.5812, 0.6207, 0.6966, 0.6650, 0.5890, 0.5102]]])\n",
      "tensor([[[0.5216, 0.4899, 0.4855],\n",
      "         [0.5564, 0.5471, 0.5223],\n",
      "         [0.5353, 0.5639, 0.5318],\n",
      "         [0.5167, 0.5793, 0.5455],\n",
      "         [0.4716, 0.5490, 0.5168],\n",
      "         [0.4680, 0.5380, 0.5174],\n",
      "         [0.4344, 0.4868, 0.4859]],\n",
      "\n",
      "        [[0.5082, 0.4820, 0.4722],\n",
      "         [0.5583, 0.5518, 0.5169],\n",
      "         [0.5599, 0.5897, 0.5451],\n",
      "         [0.5863, 0.6602, 0.6123],\n",
      "         [0.5748, 0.6837, 0.6245],\n",
      "         [0.5894, 0.6995, 0.6454],\n",
      "         [0.5448, 0.6458, 0.6135]],\n",
      "\n",
      "        [[0.4710, 0.4087, 0.3955],\n",
      "         [0.5067, 0.4283, 0.3981],\n",
      "         [0.4942, 0.4232, 0.3911],\n",
      "         [0.5590, 0.5226, 0.4965],\n",
      "         [0.6124, 0.6307, 0.5864],\n",
      "         [0.6288, 0.6961, 0.6434],\n",
      "         [0.5865, 0.6625, 0.6221]],\n",
      "\n",
      "        [[0.4613, 0.4011, 0.4139],\n",
      "         [0.4849, 0.3911, 0.3902],\n",
      "         [0.4238, 0.2941, 0.2937],\n",
      "         [0.4466, 0.2801, 0.2976],\n",
      "         [0.4884, 0.3454, 0.3515],\n",
      "         [0.5480, 0.4737, 0.4620],\n",
      "         [0.5525, 0.5306, 0.5142]],\n",
      "\n",
      "        [[0.4531, 0.4518, 0.4658],\n",
      "         [0.5083, 0.4842, 0.4808],\n",
      "         [0.4590, 0.4118, 0.4056],\n",
      "         [0.4340, 0.3183, 0.3378],\n",
      "         [0.3869, 0.2340, 0.2605],\n",
      "         [0.4033, 0.2535, 0.2780],\n",
      "         [0.4516, 0.3518, 0.3591]],\n",
      "\n",
      "        [[0.4771, 0.5183, 0.5383],\n",
      "         [0.5497, 0.6014, 0.5957],\n",
      "         [0.5646, 0.6266, 0.6098],\n",
      "         [0.5864, 0.6254, 0.6112],\n",
      "         [0.5306, 0.5223, 0.5151],\n",
      "         [0.4522, 0.3980, 0.3955],\n",
      "         [0.4601, 0.3885, 0.3915]],\n",
      "\n",
      "        [[0.4304, 0.4621, 0.5014],\n",
      "         [0.5027, 0.5618, 0.5812],\n",
      "         [0.5219, 0.6192, 0.6207],\n",
      "         [0.5721, 0.6895, 0.6966],\n",
      "         [0.5580, 0.6572, 0.6650],\n",
      "         [0.5262, 0.5823, 0.5890],\n",
      "         [0.4961, 0.5073, 0.5102]]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1700 with 64 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ensure it's in evaluation mode for visualization\n",
    "model_res50.eval()  # This changes the model to evaluation mode\n",
    "\n",
    "# Get the weights of the first convolutional layer\n",
    "weights = model_res50.conv1.weight.data.cpu()\n",
    "\n",
    "# Normalize the weights for better visualization\n",
    "weights = (weights - weights.min()) / (weights.max() - weights.min())\n",
    "print(weights[0])\n",
    "print(weights[0].permute(1, 2, 0))\n",
    "# Plot the first few filters\n",
    "plt.figure(figsize=(20, 17))\n",
    "for i in range(64):  # Let's visualize 64 filters\n",
    "    plt.subplot(8, 8, i + 1)\n",
    "    plt.imshow(weights[i].permute(1, 2, 0))  # Rearrange the dimensions\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we switch model to eval mode and define a method to test on full sized testing images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remember, since we now want to switch to our test data, we need to create a new Lyft Dataset object that points to the test data instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JSON file instance.json missing, using empty list\n",
      "JSON file sample_annotation.json missing, using empty list\n",
      "9 category,\n",
      "17 attribute,\n",
      "4 visibility,\n",
      "0 instance,\n",
      "8 sensor,\n",
      "168 calibrated_sensor,\n",
      "219744 ego_pose,\n",
      "218 log,\n",
      "218 scene,\n",
      "27468 sample,\n",
      "219744 sample_data,\n",
      "0 sample_annotation,\n",
      "1 map,\n",
      "Done loading in 2.3 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 0.8 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "data_test_path = Path(r\"S:\\MADS\\Capstone\\3d-object-detection-for-autonomous-vehicles\\Test\")\n",
    "json_test_path = r\"S:\\MADS\\Capstone\\3d-object-detection-for-autonomous-vehicles\\Test\\data\"\n",
    "lyftdata_test = LyftDataset(data_path=data_test_path, json_path=json_test_path, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'token': 'a557426183b9070f94dcee13da60e9651179320651fe0547409067c08d633ffd',\n",
       " 'timestamp': 1553537689401841.5,\n",
       " 'next': '417ee1affe8b168760a67563e0052eb5e206a1b46482ce6a2266883bccbdfe8a',\n",
       " 'scene_token': '6f4302e48a14c58ad3f0ff97dea87d28080dfdf21ec28df8875ba633556f8201',\n",
       " 'prev': '7465223ebe8ace84c9da7ae3e60721b5578744ff988d73eaa5eb43b16b68d080',\n",
       " 'data': {'CAM_FRONT_ZOOMED': '1b568d53563416f03ecaca5243d9e704049915e29a42f56014534223e8aeebf1',\n",
       "  'LIDAR_TOP': '656cbcd3e5826a1c670df251f25ef95d26f69cd37327a1f08236f810aa1b9d26',\n",
       "  'CAM_BACK_LEFT': '1aec0e7c875ca1c1881a4750def2ec41f3f27225e734b29f718c5043909aa2d8',\n",
       "  'CAM_FRONT_RIGHT': 'b85cea2a6dda0f69020d3fbb17c42b19fba9cbbc26c9f1af8d2e4defab96bc6b',\n",
       "  'CAM_BACK': '5c049a71c12a99ea55ef7462259435407e0655b13ee9082ee9e06639fdbf4457',\n",
       "  'CAM_BACK_RIGHT': '6f1bd12c7664744fed7fdf2a18f708951b514379d37343157ef26c6e24fe0409',\n",
       "  'CAM_FRONT_LEFT': '4d206c830a484d5d149c5e5b9f4529b6b1b7c765edde3b519a25344a830d446d',\n",
       "  'CAM_FRONT': '57dca6fcf86a85f0827438704e67897903d61f6245e80f825334fc368648dc4c'},\n",
       " 'anns': []}"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lyftdata_test.sample[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pedestrian_images_boxes = img_class_selector(\"pedestrian\", forced_data_path=r\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (1927823325.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[19], line 2\u001b[1;36m\u001b[0m\n\u001b[1;33m    #split up the image into\u001b[0m\n\u001b[1;37m                             ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# window_size = 224\n",
    "#Our test data/images will be preselected to only data that we know contains at least one instance of the target class.\n",
    "#Additionally, we will only account for preexisting bounding boxes that are also of the target class. This simplifies the initial task, and can be expanded to include non-target class boxes for additional testing in the future.\n",
    "#test_data will be of the form below which is a dictionaries with key as the img_id_name and the value as a list of 1:n dictionaries where each dictionary has two elements for the original and predicted bounding box whose\n",
    "#values are simply the coordinates of the bounding box in image coordinates.\n",
    "#Remember: All of our bounding_box_original boxes will be from the set of only the boxes of the target class.\n",
    "#test_data = {img_id_name: [{bounding_box_original: np.array(), bounding_box_predicted: np.array()},\n",
    "#                       {bounding_box_original: np.array(), bounding_box_predicted: np.array()}]}\n",
    "\n",
    "# for img_id_name, box_list in test_data:\n",
    "    #split up the image into \"window_size\" subsections. Input windows are 1024x1224, so window size of 224 will not split evenly. How to handle non-even splits for convolution/sliding windows?\n",
    "    #for each subsection:\n",
    "        #feed the subsection into my network and get a binary prediction: 1 - yes, this is the target class (pedestrain) or 0, no this is not the target class.\n",
    "\n",
    "        #if we found a target class:\n",
    "            #target_box = coordinates of the image that pertain to subsection that has a positive prediction\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
