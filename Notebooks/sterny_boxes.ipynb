{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "import random\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the SDK\n",
    "import lyft_dataset_sdk\n",
    "from lyft_dataset_sdk.lyftdataset import LyftDataset, LyftDatasetExplorer, Quaternion, view_points\n",
    "from lyft_dataset_sdk.utils.data_classes import LidarPointCloud\n",
    "from lyft_dataset_sdk.utils.geometry_utils import BoxVisibility, box_in_image, view_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 category,\n",
      "18 attribute,\n",
      "4 visibility,\n",
      "18421 instance,\n",
      "10 sensor,\n",
      "148 calibrated_sensor,\n",
      "177789 ego_pose,\n",
      "180 log,\n",
      "180 scene,\n",
      "22680 sample,\n",
      "189504 sample_data,\n",
      "638179 sample_annotation,\n",
      "1 map,\n",
      "Done loading in 8.3 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 2.6 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "data_path = Path(r\"S:\\MADS\\Capstone\\3d-object-detection-for-autonomous-vehicles\")\n",
    "json_path = r\"S:\\MADS\\Capstone\\3d-object-detection-for-autonomous-vehicles\\data\"\n",
    "lyftdata = LyftDataset(data_path=data_path, json_path=json_path, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Begin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to find the image files that contain our moto_anns, we must do the following:\n",
    "#Accumulate the superset of Samples that contain all of our moto_annotations\n",
    "def img_class_selector(class_name, box_visibility = BoxVisibility.ALL):\n",
    "    \"\"\"\n",
    "    There wasn't a super clean way in the API to select image files that contain annotations from x class. I adapted the method they use in their .render_annotation() method.\n",
    "\n",
    "    Returns tuples of (file_path, Box object, camera_intrinsic, sample_token, cam)\n",
    "    \"\"\"\n",
    "\n",
    "    annos_class = lyftdata.sample_annotation\n",
    "    class_anns = [d for d in annos_class if d[\"category_name\"] == class_name]\n",
    "    sample_class_tokens = set()\n",
    "    for ann in class_anns:\n",
    "        sample_class_tokens.add(ann[\"sample_token\"])\n",
    "\n",
    "    class_images_boxes = []\n",
    "    for sample_token in sample_class_tokens:\n",
    "        sample_record = lyftdata.get(\"sample\", sample_token)\n",
    "        sample_anns = sample_record[\"anns\"]\n",
    "        cams = [key for key in sample_record[\"data\"].keys() if \"CAM\" in key]\n",
    "        \n",
    "        #We unfortunately do have to iterate through each annotation that exists within the sample_record. There typically aren't more than 20-30 or so.\n",
    "        for ann in sample_anns:\n",
    "            # Figure out which camera the object is fully visible in (this may return nothing)\n",
    "            for cam in cams:\n",
    "                path, box, camera_intrinsic = lyftdata.get_sample_data(sample_record[\"data\"][cam], box_vis_level=box_visibility, selected_anntokens=[ann])\n",
    "                if box:\n",
    "                    if box[0].name == class_name:\n",
    "                        class_images_boxes.append((str(path), box[0], camera_intrinsic, sample_token, cam, ann))\n",
    "\n",
    "    return class_images_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_select_images(img_class_selector_output, output_path, box_area_threshold = 2000):\n",
    "    path, box, camera_intrinsic, sample_token, cam, ann = img_class_selector_output\n",
    "    box_coords = view_points(box.corners(), view = camera_intrinsic, normalize=True)[:2, :]\n",
    "    \n",
    "    # Calculate bounding box coordinates\n",
    "    x_min = np.min(box_coords[0])\n",
    "    y_min = np.min(box_coords[1])\n",
    "    x_max = np.max(box_coords[0])\n",
    "    y_max = np.max(box_coords[1])\n",
    "\n",
    "    # Calculate the area of the bounding box\n",
    "    box_area = (x_max - x_min) * (y_max - y_min)\n",
    "\n",
    "    if box_area > box_area_threshold:\n",
    "        im = Image.open(path)\n",
    "\n",
    "        match = re.search(r'images\\\\(.+)', path)\n",
    "\n",
    "        if match:\n",
    "            file_name = match.group(1)\n",
    "        else:\n",
    "            print(\"Pattern not found in the path\", path)\n",
    "            pass\n",
    "\n",
    "        # Crop the image\n",
    "        # Pillow uses a system of (left, upper, right, lower)\n",
    "        cropped_im = im.crop((x_min, y_min, x_max, y_max))\n",
    "\n",
    "        # Check if the directory exists, if not, create it\n",
    "        Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        #Save the image\n",
    "        cropped_im.save(os.path.join(output_path, \"cropped_\" + file_name[:-5] + \"_\" + ann + file_name[-5:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car',\n",
       " 'pedestrian',\n",
       " 'animal',\n",
       " 'other_vehicle',\n",
       " 'bus',\n",
       " 'motorcycle',\n",
       " 'truck',\n",
       " 'emergency_vehicle',\n",
       " 'bicycle']"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = [lyftdata.category[i][\"name\"] for i in range(len(lyftdata.category))]\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Object of type Box is not JSON serializable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32ms:\\MADS\\Capstone\\3d-object-detection-for-autonomous-vehicles\\Notebooks\\sterny_boxes.ipynb Cell 7\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/s%3A/MADS/Capstone/3d-object-detection-for-autonomous-vehicles/Notebooks/sterny_boxes.ipynb#X22sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39m# Write to a JSON file at the specified path\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/s%3A/MADS/Capstone/3d-object-detection-for-autonomous-vehicles/Notebooks/sterny_boxes.ipynb#X22sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(data_output_path, \u001b[39m'\u001b[39m\u001b[39mw\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[1;32m---> <a href='vscode-notebook-cell:/s%3A/MADS/Capstone/3d-object-detection-for-autonomous-vehicles/Notebooks/sterny_boxes.ipynb#X22sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     json\u001b[39m.\u001b[39;49mdump(cat_images_boxes, file, indent\u001b[39m=\u001b[39;49m\u001b[39m4\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\stern\\ytorch_env\\lib\\json\\__init__.py:179\u001b[0m, in \u001b[0;36mdump\u001b[1;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[0;32m    173\u001b[0m     iterable \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(skipkeys\u001b[39m=\u001b[39mskipkeys, ensure_ascii\u001b[39m=\u001b[39mensure_ascii,\n\u001b[0;32m    174\u001b[0m         check_circular\u001b[39m=\u001b[39mcheck_circular, allow_nan\u001b[39m=\u001b[39mallow_nan, indent\u001b[39m=\u001b[39mindent,\n\u001b[0;32m    175\u001b[0m         separators\u001b[39m=\u001b[39mseparators,\n\u001b[0;32m    176\u001b[0m         default\u001b[39m=\u001b[39mdefault, sort_keys\u001b[39m=\u001b[39msort_keys, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\u001b[39m.\u001b[39miterencode(obj)\n\u001b[0;32m    177\u001b[0m \u001b[39m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[0;32m    178\u001b[0m \u001b[39m# a debuggability cost\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m \u001b[39mfor\u001b[39;00m chunk \u001b[39min\u001b[39;00m iterable:\n\u001b[0;32m    180\u001b[0m     fp\u001b[39m.\u001b[39mwrite(chunk)\n",
      "File \u001b[1;32mc:\\Users\\stern\\ytorch_env\\lib\\json\\encoder.py:429\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    427\u001b[0m     \u001b[39myield\u001b[39;00m _floatstr(o)\n\u001b[0;32m    428\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(o, (\u001b[39mlist\u001b[39m, \u001b[39mtuple\u001b[39m)):\n\u001b[1;32m--> 429\u001b[0m     \u001b[39myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[0;32m    430\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(o, \u001b[39mdict\u001b[39m):\n\u001b[0;32m    431\u001b[0m     \u001b[39myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n",
      "File \u001b[1;32mc:\\Users\\stern\\ytorch_env\\lib\\json\\encoder.py:325\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_list\u001b[1;34m(lst, _current_indent_level)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    324\u001b[0m             chunks \u001b[39m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[1;32m--> 325\u001b[0m         \u001b[39myield from\u001b[39;00m chunks\n\u001b[0;32m    326\u001b[0m \u001b[39mif\u001b[39;00m newline_indent \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    327\u001b[0m     _current_indent_level \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\stern\\ytorch_env\\lib\\json\\encoder.py:325\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_list\u001b[1;34m(lst, _current_indent_level)\u001b[0m\n\u001b[0;32m    323\u001b[0m         \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    324\u001b[0m             chunks \u001b[39m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[1;32m--> 325\u001b[0m         \u001b[39myield from\u001b[39;00m chunks\n\u001b[0;32m    326\u001b[0m \u001b[39mif\u001b[39;00m newline_indent \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    327\u001b[0m     _current_indent_level \u001b[39m-\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\stern\\ytorch_env\\lib\\json\\encoder.py:438\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[1;34m(o, _current_indent_level)\u001b[0m\n\u001b[0;32m    436\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mCircular reference detected\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    437\u001b[0m     markers[markerid] \u001b[39m=\u001b[39m o\n\u001b[1;32m--> 438\u001b[0m o \u001b[39m=\u001b[39m _default(o)\n\u001b[0;32m    439\u001b[0m \u001b[39myield from\u001b[39;00m _iterencode(o, _current_indent_level)\n\u001b[0;32m    440\u001b[0m \u001b[39mif\u001b[39;00m markers \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\stern\\ytorch_env\\lib\\json\\encoder.py:179\u001b[0m, in \u001b[0;36mJSONEncoder.default\u001b[1;34m(self, o)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault\u001b[39m(\u001b[39mself\u001b[39m, o):\n\u001b[0;32m    161\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Implement this method in a subclass such that it returns\u001b[39;00m\n\u001b[0;32m    162\u001b[0m \u001b[39m    a serializable object for ``o``, or calls the base implementation\u001b[39;00m\n\u001b[0;32m    163\u001b[0m \u001b[39m    (to raise a ``TypeError``).\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    177\u001b[0m \n\u001b[0;32m    178\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 179\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mObject of type \u001b[39m\u001b[39m{\u001b[39;00mo\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    180\u001b[0m                     \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mis not JSON serializable\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: Object of type Box is not JSON serializable"
     ]
    }
   ],
   "source": [
    "for cat in categories:\n",
    "\n",
    "    #We can skip car because we know we aren't doing this one, plus its by far the largest and most timely.\n",
    "    if cat != \"car\":\n",
    "        #Obtain all of the annotations and their corresponding data via image_class_selector\n",
    "        cat_images_boxes = img_class_selector(cat)\n",
    "        img_output_path = os.path.join(r\"S:\\MADS\\Capstone\\3d-object-detection-for-autonomous-vehicles\\images\", categories[0]+\"_cropped\")\n",
    "\n",
    "        #Do the actual cropping and write cropped images to disk\n",
    "        for cat_images_box in cat_images_boxes:\n",
    "            crop_select_images(cat_images_box, output_path=img_output_path, box_area_threshold=5000)\n",
    "\n",
    "\n",
    "        #Since we went through the effort of running img_class_selector, lets write this to disk for each category so we don't have to run every time.\n",
    "\n",
    "        # Convert NumPy arrays to lists (if any)\n",
    "        # For example, if 'camera_intrinsic' is a NumPy array:\n",
    "        cat_images_boxes['motorcycle'][0]['camera_intrinsic'] = cat_images_boxes['motorcycle'][0]['camera_intrinsic'].tolist()\n",
    "        data_output_path = os.path.join(r\"S:\\MADS\\Capstone\\3d-object-detection-for-autonomous-vehicles\\sterny_data\", categories[0], \".json\")\n",
    "        \n",
    "        # Check if the directory exists, if not, create it\n",
    "        output_dir = os.path.dirname(data_output_path)\n",
    "        Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        # Write to a JSON file at the specified path\n",
    "        with open(data_output_path, 'w') as file:\n",
    "            json.dump(cat_images_boxes, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_path = r\"S:\\MADS\\Capstone\\3d-object-detection-for-autonomous-vehicles\\images\\moto_cropped\"\n",
    "for moto_img_box in motorcycle_images_boxes:\n",
    "    crop_select_images(moto_img_box, output_path=output_path, box_area_threshold=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionError",
     "evalue": "[Errno 13] Permission denied: 'S:\\\\MADS\\\\Capstone\\\\3d-object-detection-for-autonomous-vehicles\\\\sterny_data'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPermissionError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32ms:\\MADS\\Capstone\\3d-object-detection-for-autonomous-vehicles\\Notebooks\\sterny_boxes.ipynb Cell 12\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/s%3A/MADS/Capstone/3d-object-detection-for-autonomous-vehicles/Notebooks/sterny_boxes.ipynb#X20sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m write_path \u001b[39m=\u001b[39m \u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mS:\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mMADS\u001b[39m\u001b[39m\\\u001b[39m\u001b[39mCapstone\u001b[39m\u001b[39m\\\u001b[39m\u001b[39m3d-object-detection-for-autonomous-vehicles\u001b[39m\u001b[39m\\\u001b[39m\u001b[39msterny_data\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m      <a href='vscode-notebook-cell:/s%3A/MADS/Capstone/3d-object-detection-for-autonomous-vehicles/Notebooks/sterny_boxes.ipynb#X20sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m# Write to a JSON file at the specified path\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/s%3A/MADS/Capstone/3d-object-detection-for-autonomous-vehicles/Notebooks/sterny_boxes.ipynb#X20sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(write_path, \u001b[39m'\u001b[39;49m\u001b[39mw\u001b[39;49m\u001b[39m'\u001b[39;49m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m      <a href='vscode-notebook-cell:/s%3A/MADS/Capstone/3d-object-detection-for-autonomous-vehicles/Notebooks/sterny_boxes.ipynb#X20sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     json\u001b[39m.\u001b[39mdump(motorcycle_images_boxes, file, indent\u001b[39m=\u001b[39m\u001b[39m4\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\stern\\ytorch_env\\lib\\site-packages\\IPython\\core\\interactiveshell.py:286\u001b[0m, in \u001b[0;36m_modified_open\u001b[1;34m(file, *args, **kwargs)\u001b[0m\n\u001b[0;32m    279\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[0;32m    280\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    281\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    282\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    283\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    284\u001b[0m     )\n\u001b[1;32m--> 286\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[1;31mPermissionError\u001b[0m: [Errno 13] Permission denied: 'S:\\\\MADS\\\\Capstone\\\\3d-object-detection-for-autonomous-vehicles\\\\sterny_data'"
     ]
    }
   ],
   "source": [
    "write_path = r\"S:\\MADS\\Capstone\\3d-object-detection-for-autonomous-vehicles\\sterny_data\"\n",
    "\n",
    "# Write to a JSON file at the specified path\n",
    "with open(write_path, 'w') as file:\n",
    "    json.dump(motorcycle_images_boxes, file, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
