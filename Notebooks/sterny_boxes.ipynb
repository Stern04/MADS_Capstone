{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import cv2\n",
    "from pathlib import Path\n",
    "from PIL import Image, ImageOps\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "import random\n",
    "import re\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import models\n",
    "from torchvision.models import resnet50\n",
    "from torchvision.io import read_image\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the SDK\n",
    "import lyft_dataset_sdk\n",
    "from lyft_dataset_sdk.lyftdataset import LyftDataset, LyftDatasetExplorer, Quaternion, view_points\n",
    "from lyft_dataset_sdk.utils.data_classes import LidarPointCloud\n",
    "from lyft_dataset_sdk.utils.geometry_utils import BoxVisibility, box_in_image, view_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9 category,\n",
      "18 attribute,\n",
      "4 visibility,\n",
      "18421 instance,\n",
      "10 sensor,\n",
      "148 calibrated_sensor,\n",
      "177789 ego_pose,\n",
      "180 log,\n",
      "180 scene,\n",
      "22680 sample,\n",
      "189504 sample_data,\n",
      "638179 sample_annotation,\n",
      "1 map,\n",
      "Done loading in 9.8 seconds.\n",
      "======\n",
      "Reverse indexing ...\n",
      "Done reverse indexing in 2.7 seconds.\n",
      "======\n"
     ]
    }
   ],
   "source": [
    "data_path = Path(r\"S:\\MADS\\Capstone\\3d-object-detection-for-autonomous-vehicles\")\n",
    "json_path = r\"S:\\MADS\\Capstone\\3d-object-detection-for-autonomous-vehicles\\data\"\n",
    "lyftdata = LyftDataset(data_path=data_path, json_path=json_path, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Selection/Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#In order to find the image files that contain our moto_anns, we must do the following:\n",
    "#Accumulate the superset of Samples that contain all of our moto_annotations\n",
    "def img_class_selector(class_name, box_visibility = BoxVisibility.ALL):\n",
    "    \"\"\"\n",
    "    There wasn't a super clean way in the API to select image files that contain annotations from x class. I adapted the method they use in their .render_annotation() method.\n",
    "\n",
    "    Returns tuples of (file_path, Box object, camera_intrinsic, sample_token, cam)\n",
    "    \"\"\"\n",
    "\n",
    "    annos_class = lyftdata.sample_annotation\n",
    "    class_anns = [d for d in annos_class if d[\"category_name\"] == class_name]\n",
    "    sample_class_tokens = set()\n",
    "    for ann in class_anns:\n",
    "        sample_class_tokens.add(ann[\"sample_token\"])\n",
    "\n",
    "    class_images_boxes = []\n",
    "    for sample_token in sample_class_tokens:\n",
    "        sample_record = lyftdata.get(\"sample\", sample_token)\n",
    "        sample_anns = sample_record[\"anns\"]\n",
    "        cams = [key for key in sample_record[\"data\"].keys() if \"CAM\" in key]\n",
    "        \n",
    "        #We unfortunately do have to iterate through each annotation that exists within the sample_record. There typically aren't more than 20-30 or so.\n",
    "        for ann in sample_anns:\n",
    "            # Figure out which camera the object is fully visible in (this may return nothing)\n",
    "            for cam in cams:\n",
    "                path, box, camera_intrinsic = lyftdata.get_sample_data(sample_record[\"data\"][cam], box_vis_level=box_visibility, selected_anntokens=[ann])\n",
    "                if box:\n",
    "                    if box[0].name == class_name:\n",
    "                        class_images_boxes.append((str(path), box[0], camera_intrinsic, sample_token, cam, ann))\n",
    "\n",
    "    return class_images_boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_select_images(img_class_selector_output, output_path, output_size ,box_area_threshold = 5000):\n",
    "    path, box, camera_intrinsic, sample_token, cam, ann = img_class_selector_output\n",
    "    \n",
    "    box_coords = view_points(box.corners(), view = camera_intrinsic, normalize=True)[:2, :]\n",
    "    \n",
    "    # Calculate bounding box coordinates\n",
    "    x_min = np.min(box_coords[0])\n",
    "    y_min = np.min(box_coords[1])\n",
    "    x_max = np.max(box_coords[0])\n",
    "    y_max = np.max(box_coords[1])\n",
    "\n",
    "    # Calculate the area of the bounding box\n",
    "    box_area = (x_max - x_min) * (y_max - y_min)\n",
    "\n",
    "    if box_area > box_area_threshold:\n",
    "        img = Image.open(path)\n",
    "        match = re.search(r'images\\\\(.+)', path)\n",
    "\n",
    "        if match:\n",
    "            file_name = match.group(1)\n",
    "        else:\n",
    "            print(\"Pattern not found in the path\", path)\n",
    "            pass\n",
    "\n",
    "        # Crop the image. This is our initial cropping from the full size image. It is with respect to the bounding box size using box_area_threshold.\n",
    "        # Pillow uses a system of (left, upper, right, lower)\n",
    "        img_cropped = img.crop((x_min, y_min, x_max, y_max))\n",
    "\n",
    "        # Check if the directory exists, if not, create it\n",
    "        Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "        #Save the image\n",
    "        img_cropped.save(os.path.join(output_path, \"cropped_\" + file_name[:-5] + \"_\" + ann + file_name[-5:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['car',\n",
       " 'pedestrian',\n",
       " 'animal',\n",
       " 'other_vehicle',\n",
       " 'bus',\n",
       " 'motorcycle',\n",
       " 'truck',\n",
       " 'emergency_vehicle',\n",
       " 'bicycle']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories = [lyftdata.category[i][\"name\"] for i in range(len(lyftdata.category))]\n",
    "categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ped_images_boxes = img_class_selector(\"pedestrian\")\n",
    "# crop_select_images(ped_images_boxes)\n",
    "\n",
    "# categories = [lyftdata.category[i][\"name\"] for i in range(len(lyftdata.category))]\n",
    "# for cat in categories:\n",
    "\n",
    "#     #We can skip car because we know we aren't doing this one, plus its by far the largest and most timely.\n",
    "#     if cat != \"car\":\n",
    "#         #Obtain all of the annotations and their corresponding data via image_class_selector\n",
    "#         cat_images_boxes = img_class_selector(cat)\n",
    "\n",
    "#         #Do the actual cropping and write cropped images to disk\n",
    "#         img_output_path = os.path.join(r\"S:\\MADS\\Capstone\\3d-object-detection-for-autonomous-vehicles\\images\", cat +\"_cropped\")\n",
    "        \n",
    "#         for i in range(len(cat_images_boxes)):\n",
    "#             crop_select_images(cat_images_boxes[i], output_path=img_output_path, box_area_threshold=5000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess pedestrain images with additional processes like Padding and Center cropping for input into model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Lyft_experimental_CustomDataset(Dataset):\n",
    "    def __init__(self, cropped_images_dir, target_class_name, required_size=224):\n",
    "        self.cropped_images_dir = cropped_images_dir\n",
    "        self.required_size = required_size\n",
    "        self.image_filenames = os.listdir(cropped_images_dir)\n",
    "        self.data = []\n",
    "        #Our class will ingest cropped images from all classes. We must add corresponding labels to these images.\n",
    "        #The images are stored in directories wrt their class. So, we use the dir name to label the image.\n",
    "        # Iterate over each subdirectory in the main directory\n",
    "        for class_name in os.listdir(cropped_images_dir):\n",
    "            class_dir = os.path.join(cropped_images_dir, class_name)\n",
    "            if os.path.isdir(class_dir):  # Check if it's a directory\n",
    "                is_target_class = class_name == target_class_name\n",
    "                label = 1 if is_target_class else 0\n",
    "\n",
    "                for filename in os.listdir(class_dir):\n",
    "                    file_path = os.path.join(class_dir, filename)\n",
    "                    if os.path.isfile(file_path):  # Check if it's a file\n",
    "                        self.data.append((file_path, label))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path, label = self.data[idx]\n",
    "        image = Image.open(img_path)\n",
    "\n",
    "        # Apply dynamic transformation based on image size\n",
    "        image = self.dynamic_transform(image)\n",
    "\n",
    "        # Convert the image to tensor and normalize\n",
    "        image = transforms.ToTensor()(image)\n",
    "        image = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])(image)\n",
    "\n",
    "        return image, label\n",
    "\n",
    "    def dynamic_transform(self, img):\n",
    "        img_width, img_height = img.size\n",
    "\n",
    "        if min(img.size) < self.required_size:\n",
    "            # Calculate padding\n",
    "            padding_left = (self.required_size - img_width) // 2\n",
    "            padding_top = (self.required_size - img_height) // 2\n",
    "\n",
    "            # Adjust for odd total padding\n",
    "            padding_right = self.required_size - img_width - padding_left\n",
    "            padding_bottom = self.required_size - img_height - padding_top\n",
    "\n",
    "            # Apply padding\n",
    "            img = ImageOps.expand(img, (padding_left, padding_top, padding_right, padding_bottom), fill=0)\n",
    "            \n",
    "        elif min(img.size) > self.required_size:\n",
    "            # Crop the image to the required size if it's larger\n",
    "            img = transforms.CenterCrop(self.required_size)(img)\n",
    "        # No else case needed, as no action is taken if the image is already the required size\n",
    "        return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "cropped_images_dir = r'S:\\MADS\\Capstone\\3d-object-detection-for-autonomous-vehicles\\images\\cropped_images'\n",
    "dataset = Lyft_experimental_CustomDataset(cropped_images_dir, target_class_name = \"pedestrian\", required_size=224)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
      "\n",
      "\n",
      "        [[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
      "\n",
      "\n",
      "        [[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]],\n",
      "\n",
      "\n",
      "        [[[-1.0390, -1.0390, -1.0048,  ..., -1.3815, -1.3644, -1.3644],\n",
      "          [-1.0390, -1.0219, -1.0390,  ..., -1.3815, -1.3644, -1.3473],\n",
      "          [-1.0390, -1.0390, -1.0390,  ..., -1.3815, -1.3644, -1.3473],\n",
      "          ...,\n",
      "          [-0.5424, -0.5424, -0.5596,  ..., -1.2103, -1.1932, -1.1760],\n",
      "          [-0.5253, -0.5253, -0.5596,  ..., -1.1760, -1.1760, -1.1589],\n",
      "          [-0.5253, -0.5253, -0.5424,  ..., -1.1932, -1.1760, -1.1418]],\n",
      "\n",
      "         [[-0.9853, -0.9853, -0.9503,  ..., -1.2654, -1.2479, -1.2479],\n",
      "          [-0.9853, -0.9678, -0.9853,  ..., -1.2654, -1.2479, -1.2304],\n",
      "          [-0.9853, -0.9853, -0.9853,  ..., -1.2654, -1.2479, -1.2304],\n",
      "          ...,\n",
      "          [-0.4251, -0.4251, -0.4426,  ..., -1.2479, -1.2304, -1.2129],\n",
      "          [-0.4076, -0.4076, -0.4426,  ..., -1.2129, -1.2129, -1.1954],\n",
      "          [-0.4076, -0.4076, -0.4251,  ..., -1.2304, -1.2129, -1.1779]],\n",
      "\n",
      "         [[-1.0550, -1.0550, -1.0201,  ..., -1.1421, -1.1247, -1.1247],\n",
      "          [-1.0550, -1.0376, -1.0550,  ..., -1.1421, -1.1247, -1.1073],\n",
      "          [-1.0550, -1.0550, -1.0550,  ..., -1.1421, -1.1247, -1.1073],\n",
      "          ...,\n",
      "          [-0.3404, -0.3404, -0.3578,  ..., -1.2119, -1.1944, -1.1770],\n",
      "          [-0.3230, -0.3230, -0.3578,  ..., -1.1770, -1.1770, -1.1596],\n",
      "          [-0.3230, -0.3230, -0.3404,  ..., -1.1944, -1.1770, -1.1421]]],\n",
      "\n",
      "\n",
      "        [[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          ...,\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "          [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "         [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          ...,\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "          [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "         [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          ...,\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "          [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]])\n",
      "tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 1, 0, 1, 0])\n"
     ]
    }
   ],
   "source": [
    "for data in dataloader:\n",
    "    print(data[0])\n",
    "    print(data[1])\n",
    "    # print(data[1][0])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch 0: Labels - tensor([0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Batch 1: Labels - tensor([0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Batch 2: Labels - tensor([1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 1, 0, 0, 1,\n",
      "        1, 0, 0, 0, 0, 0, 0, 0])\n",
      "Batch 3: Labels - tensor([1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 1, 0, 0, 0, 0, 0, 0])\n",
      "Batch 4: Labels - tensor([0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n",
      "Batch 5: Labels - tensor([1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0])\n"
     ]
    }
   ],
   "source": [
    "for i, data in enumerate(dataloader):\n",
    "    print(f\"Batch {i}: Labels - {data[1]}\")\n",
    "    if i == 5:  # Check first 5 batches\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         ...,\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "        [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         ...,\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "        [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         ...,\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]), 0)\n",
      "(tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         ...,\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "        [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         ...,\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "        [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         ...,\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]), 0)\n",
      "(tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         ...,\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "        [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         ...,\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "        [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         ...,\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]), 0)\n",
      "(tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         ...,\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "        [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         ...,\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "        [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         ...,\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]), 0)\n",
      "(tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         ...,\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "        [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         ...,\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "        [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         ...,\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]), 0)\n",
      "(tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         ...,\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "        [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         ...,\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "        [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         ...,\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]), 0)\n",
      "(tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         ...,\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "        [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         ...,\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "        [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         ...,\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]), 0)\n",
      "(tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         ...,\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "        [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         ...,\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "        [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         ...,\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]), 0)\n",
      "(tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         ...,\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "        [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         ...,\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "        [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         ...,\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]), 0)\n",
      "(tensor([[[-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         ...,\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179],\n",
      "         [-2.1179, -2.1179, -2.1179,  ..., -2.1179, -2.1179, -2.1179]],\n",
      "\n",
      "        [[-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         ...,\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357],\n",
      "         [-2.0357, -2.0357, -2.0357,  ..., -2.0357, -2.0357, -2.0357]],\n",
      "\n",
      "        [[-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         ...,\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044],\n",
      "         [-1.8044, -1.8044, -1.8044,  ..., -1.8044, -1.8044, -1.8044]]]), 0)\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):  # Test the first 10 items\n",
    "    try:\n",
    "        print(dataset[i])\n",
    "    except Exception as e:\n",
    "        print(f\"Error at index {i}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Pytorch models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\stern\\ytorch_env\\lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\stern\\ytorch_env\\lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2048\n"
     ]
    }
   ],
   "source": [
    "model_res50 = models.resnet50(pretrained=True)\n",
    "num_ftrs = model_res50.fc.in_features\n",
    "print(num_ftrs)\n",
    "model_res50.fc = nn.Linear(num_ftrs, 2) # Modify the last layer for binary classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Define loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model_res50.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check if GPU is available and move the model to GPU if it is"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "model_res50 = model_res50.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train through n epochs. In each epoch, we set the model into train mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "stack expects each tensor to be equal size, but got [3, 224, 224] at entry 0 and [3, 224, 238] at entry 7",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32ms:\\MADS\\Capstone\\3d-object-detection-for-autonomous-vehicles\\Notebooks\\sterny_boxes.ipynb Cell 21\u001b[0m line \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/s%3A/MADS/Capstone/3d-object-detection-for-autonomous-vehicles/Notebooks/sterny_boxes.ipynb#X21sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m running_corrects \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/s%3A/MADS/Capstone/3d-object-detection-for-autonomous-vehicles/Notebooks/sterny_boxes.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39m# Iterate over data\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/s%3A/MADS/Capstone/3d-object-detection-for-autonomous-vehicles/Notebooks/sterny_boxes.ipynb#X21sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39mfor\u001b[39;00m inputs, labels \u001b[39min\u001b[39;00m dataloader:\n\u001b[0;32m     <a href='vscode-notebook-cell:/s%3A/MADS/Capstone/3d-object-detection-for-autonomous-vehicles/Notebooks/sterny_boxes.ipynb#X21sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     inputs \u001b[39m=\u001b[39m inputs\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/s%3A/MADS/Capstone/3d-object-detection-for-autonomous-vehicles/Notebooks/sterny_boxes.ipynb#X21sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n",
      "File \u001b[1;32mc:\\Users\\stern\\ytorch_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:630\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sampler_iter \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    628\u001b[0m     \u001b[39m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    629\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_reset()  \u001b[39m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 630\u001b[0m data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_next_data()\n\u001b[0;32m    631\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[0;32m    632\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dataset_kind \u001b[39m==\u001b[39m _DatasetKind\u001b[39m.\u001b[39mIterable \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    633\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \\\n\u001b[0;32m    634\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_yielded \u001b[39m>\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32mc:\\Users\\stern\\ytorch_env\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:674\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    672\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_next_data\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    673\u001b[0m     index \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_next_index()  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 674\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dataset_fetcher\u001b[39m.\u001b[39;49mfetch(index)  \u001b[39m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    675\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory:\n\u001b[0;32m    676\u001b[0m         data \u001b[39m=\u001b[39m _utils\u001b[39m.\u001b[39mpin_memory\u001b[39m.\u001b[39mpin_memory(data, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32mc:\\Users\\stern\\ytorch_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:54\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     52\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     53\u001b[0m     data \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 54\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcollate_fn(data)\n",
      "File \u001b[1;32mc:\\Users\\stern\\ytorch_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:270\u001b[0m, in \u001b[0;36mdefault_collate\u001b[1;34m(batch)\u001b[0m\n\u001b[0;32m    209\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdefault_collate\u001b[39m(batch):\n\u001b[0;32m    210\u001b[0m \u001b[39m    \u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    211\u001b[0m \u001b[39m        Function that takes in a batch of data and puts the elements within the batch\u001b[39;00m\n\u001b[0;32m    212\u001b[0m \u001b[39m        into a tensor with an additional outer dimension - batch size. The exact output type can be\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[39m            >>> default_collate(batch)  # Handle `CustomType` automatically\u001b[39;00m\n\u001b[0;32m    269\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 270\u001b[0m     \u001b[39mreturn\u001b[39;00m collate(batch, collate_fn_map\u001b[39m=\u001b[39;49mdefault_collate_fn_map)\n",
      "File \u001b[1;32mc:\\Users\\stern\\ytorch_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\stern\\ytorch_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:142\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    139\u001b[0m transposed \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mbatch))  \u001b[39m# It may be accessed twice, so we use a list.\u001b[39;00m\n\u001b[0;32m    141\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m--> 142\u001b[0m     \u001b[39mreturn\u001b[39;00m [collate(samples, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map) \u001b[39mfor\u001b[39;00m samples \u001b[39min\u001b[39;00m transposed]  \u001b[39m# Backwards compatibility.\u001b[39;00m\n\u001b[0;32m    143\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\stern\\ytorch_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:119\u001b[0m, in \u001b[0;36mcollate\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[39mif\u001b[39;00m collate_fn_map \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    118\u001b[0m     \u001b[39mif\u001b[39;00m elem_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[1;32m--> 119\u001b[0m         \u001b[39mreturn\u001b[39;00m collate_fn_map[elem_type](batch, collate_fn_map\u001b[39m=\u001b[39;49mcollate_fn_map)\n\u001b[0;32m    121\u001b[0m     \u001b[39mfor\u001b[39;00m collate_type \u001b[39min\u001b[39;00m collate_fn_map:\n\u001b[0;32m    122\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(elem, collate_type):\n",
      "File \u001b[1;32mc:\\Users\\stern\\ytorch_env\\lib\\site-packages\\torch\\utils\\data\\_utils\\collate.py:167\u001b[0m, in \u001b[0;36mcollate_tensor_fn\u001b[1;34m(batch, collate_fn_map)\u001b[0m\n\u001b[0;32m    165\u001b[0m     storage \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39m_typed_storage()\u001b[39m.\u001b[39m_new_shared(numel, device\u001b[39m=\u001b[39melem\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m    166\u001b[0m     out \u001b[39m=\u001b[39m elem\u001b[39m.\u001b[39mnew(storage)\u001b[39m.\u001b[39mresize_(\u001b[39mlen\u001b[39m(batch), \u001b[39m*\u001b[39m\u001b[39mlist\u001b[39m(elem\u001b[39m.\u001b[39msize()))\n\u001b[1;32m--> 167\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mstack(batch, \u001b[39m0\u001b[39;49m, out\u001b[39m=\u001b[39;49mout)\n",
      "\u001b[1;31mRuntimeError\u001b[0m: stack expects each tensor to be equal size, but got [3, 224, 224] at entry 0 and [3, 224, 238] at entry 7"
     ]
    }
   ],
   "source": [
    "# Number of epochs to train for\n",
    "num_epochs = 10\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    #Set training mode at beginning of each epoch in case we train on validation data within epoch with requires model.eval()\n",
    "    model_res50.train()  # Set the model to training mode\n",
    "\n",
    "    running_loss = 0.0\n",
    "    running_corrects = 0\n",
    "\n",
    "    # Iterate over data\n",
    "    for inputs, labels in dataloader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model_res50(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass and optimize\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Statistics\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "    epoch_loss = running_loss / len(dataset)\n",
    "    epoch_acc = running_corrects.double() / len(dataset)\n",
    "\n",
    "    print(f'Epoch {epoch}/{num_epochs - 1} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer: conv1 | Kernel Size: (7, 7) | Output Channels: 64\n",
      "Layer: layer1.0.conv1 | Kernel Size: (1, 1) | Output Channels: 64\n",
      "Layer: layer1.0.conv2 | Kernel Size: (3, 3) | Output Channels: 64\n",
      "Layer: layer1.0.conv3 | Kernel Size: (1, 1) | Output Channels: 256\n",
      "Layer: layer1.0.downsample.0 | Kernel Size: (1, 1) | Output Channels: 256\n",
      "Layer: layer1.1.conv1 | Kernel Size: (1, 1) | Output Channels: 64\n",
      "Layer: layer1.1.conv2 | Kernel Size: (3, 3) | Output Channels: 64\n",
      "Layer: layer1.1.conv3 | Kernel Size: (1, 1) | Output Channels: 256\n",
      "Layer: layer1.2.conv1 | Kernel Size: (1, 1) | Output Channels: 64\n",
      "Layer: layer1.2.conv2 | Kernel Size: (3, 3) | Output Channels: 64\n",
      "Layer: layer1.2.conv3 | Kernel Size: (1, 1) | Output Channels: 256\n",
      "Layer: layer2.0.conv1 | Kernel Size: (1, 1) | Output Channels: 128\n",
      "Layer: layer2.0.conv2 | Kernel Size: (3, 3) | Output Channels: 128\n",
      "Layer: layer2.0.conv3 | Kernel Size: (1, 1) | Output Channels: 512\n",
      "Layer: layer2.0.downsample.0 | Kernel Size: (1, 1) | Output Channels: 512\n",
      "Layer: layer2.1.conv1 | Kernel Size: (1, 1) | Output Channels: 128\n",
      "Layer: layer2.1.conv2 | Kernel Size: (3, 3) | Output Channels: 128\n",
      "Layer: layer2.1.conv3 | Kernel Size: (1, 1) | Output Channels: 512\n",
      "Layer: layer2.2.conv1 | Kernel Size: (1, 1) | Output Channels: 128\n",
      "Layer: layer2.2.conv2 | Kernel Size: (3, 3) | Output Channels: 128\n",
      "Layer: layer2.2.conv3 | Kernel Size: (1, 1) | Output Channels: 512\n",
      "Layer: layer2.3.conv1 | Kernel Size: (1, 1) | Output Channels: 128\n",
      "Layer: layer2.3.conv2 | Kernel Size: (3, 3) | Output Channels: 128\n",
      "Layer: layer2.3.conv3 | Kernel Size: (1, 1) | Output Channels: 512\n",
      "Layer: layer3.0.conv1 | Kernel Size: (1, 1) | Output Channels: 256\n",
      "Layer: layer3.0.conv2 | Kernel Size: (3, 3) | Output Channels: 256\n",
      "Layer: layer3.0.conv3 | Kernel Size: (1, 1) | Output Channels: 1024\n",
      "Layer: layer3.0.downsample.0 | Kernel Size: (1, 1) | Output Channels: 1024\n",
      "Layer: layer3.1.conv1 | Kernel Size: (1, 1) | Output Channels: 256\n",
      "Layer: layer3.1.conv2 | Kernel Size: (3, 3) | Output Channels: 256\n",
      "Layer: layer3.1.conv3 | Kernel Size: (1, 1) | Output Channels: 1024\n",
      "Layer: layer3.2.conv1 | Kernel Size: (1, 1) | Output Channels: 256\n",
      "Layer: layer3.2.conv2 | Kernel Size: (3, 3) | Output Channels: 256\n",
      "Layer: layer3.2.conv3 | Kernel Size: (1, 1) | Output Channels: 1024\n",
      "Layer: layer3.3.conv1 | Kernel Size: (1, 1) | Output Channels: 256\n",
      "Layer: layer3.3.conv2 | Kernel Size: (3, 3) | Output Channels: 256\n",
      "Layer: layer3.3.conv3 | Kernel Size: (1, 1) | Output Channels: 1024\n",
      "Layer: layer3.4.conv1 | Kernel Size: (1, 1) | Output Channels: 256\n",
      "Layer: layer3.4.conv2 | Kernel Size: (3, 3) | Output Channels: 256\n",
      "Layer: layer3.4.conv3 | Kernel Size: (1, 1) | Output Channels: 1024\n",
      "Layer: layer3.5.conv1 | Kernel Size: (1, 1) | Output Channels: 256\n",
      "Layer: layer3.5.conv2 | Kernel Size: (3, 3) | Output Channels: 256\n",
      "Layer: layer3.5.conv3 | Kernel Size: (1, 1) | Output Channels: 1024\n",
      "Layer: layer4.0.conv1 | Kernel Size: (1, 1) | Output Channels: 512\n",
      "Layer: layer4.0.conv2 | Kernel Size: (3, 3) | Output Channels: 512\n",
      "Layer: layer4.0.conv3 | Kernel Size: (1, 1) | Output Channels: 2048\n",
      "Layer: layer4.0.downsample.0 | Kernel Size: (1, 1) | Output Channels: 2048\n",
      "Layer: layer4.1.conv1 | Kernel Size: (1, 1) | Output Channels: 512\n",
      "Layer: layer4.1.conv2 | Kernel Size: (3, 3) | Output Channels: 512\n",
      "Layer: layer4.1.conv3 | Kernel Size: (1, 1) | Output Channels: 2048\n",
      "Layer: layer4.2.conv1 | Kernel Size: (1, 1) | Output Channels: 512\n",
      "Layer: layer4.2.conv2 | Kernel Size: (3, 3) | Output Channels: 512\n",
      "Layer: layer4.2.conv3 | Kernel Size: (1, 1) | Output Channels: 2048\n"
     ]
    }
   ],
   "source": [
    "for name, layer in model_res50.named_modules():\n",
    "    if isinstance(layer, nn.Conv2d):\n",
    "        print(f\"Layer: {name} | Kernel Size: {layer.kernel_size} | Output Channels: {layer.out_channels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_dict = model_res50.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['conv1.weight',\n",
       " 'bn1.weight',\n",
       " 'bn1.bias',\n",
       " 'bn1.running_mean',\n",
       " 'bn1.running_var',\n",
       " 'bn1.num_batches_tracked',\n",
       " 'layer1.0.conv1.weight',\n",
       " 'layer1.0.bn1.weight',\n",
       " 'layer1.0.bn1.bias',\n",
       " 'layer1.0.bn1.running_mean',\n",
       " 'layer1.0.bn1.running_var',\n",
       " 'layer1.0.bn1.num_batches_tracked',\n",
       " 'layer1.0.conv2.weight',\n",
       " 'layer1.0.bn2.weight',\n",
       " 'layer1.0.bn2.bias',\n",
       " 'layer1.0.bn2.running_mean',\n",
       " 'layer1.0.bn2.running_var',\n",
       " 'layer1.0.bn2.num_batches_tracked',\n",
       " 'layer1.0.conv3.weight',\n",
       " 'layer1.0.bn3.weight',\n",
       " 'layer1.0.bn3.bias',\n",
       " 'layer1.0.bn3.running_mean',\n",
       " 'layer1.0.bn3.running_var',\n",
       " 'layer1.0.bn3.num_batches_tracked',\n",
       " 'layer1.0.downsample.0.weight',\n",
       " 'layer1.0.downsample.1.weight',\n",
       " 'layer1.0.downsample.1.bias',\n",
       " 'layer1.0.downsample.1.running_mean',\n",
       " 'layer1.0.downsample.1.running_var',\n",
       " 'layer1.0.downsample.1.num_batches_tracked',\n",
       " 'layer1.1.conv1.weight',\n",
       " 'layer1.1.bn1.weight',\n",
       " 'layer1.1.bn1.bias',\n",
       " 'layer1.1.bn1.running_mean',\n",
       " 'layer1.1.bn1.running_var',\n",
       " 'layer1.1.bn1.num_batches_tracked',\n",
       " 'layer1.1.conv2.weight',\n",
       " 'layer1.1.bn2.weight',\n",
       " 'layer1.1.bn2.bias',\n",
       " 'layer1.1.bn2.running_mean',\n",
       " 'layer1.1.bn2.running_var',\n",
       " 'layer1.1.bn2.num_batches_tracked',\n",
       " 'layer1.1.conv3.weight',\n",
       " 'layer1.1.bn3.weight',\n",
       " 'layer1.1.bn3.bias',\n",
       " 'layer1.1.bn3.running_mean',\n",
       " 'layer1.1.bn3.running_var',\n",
       " 'layer1.1.bn3.num_batches_tracked',\n",
       " 'layer1.2.conv1.weight',\n",
       " 'layer1.2.bn1.weight',\n",
       " 'layer1.2.bn1.bias',\n",
       " 'layer1.2.bn1.running_mean',\n",
       " 'layer1.2.bn1.running_var',\n",
       " 'layer1.2.bn1.num_batches_tracked',\n",
       " 'layer1.2.conv2.weight',\n",
       " 'layer1.2.bn2.weight',\n",
       " 'layer1.2.bn2.bias',\n",
       " 'layer1.2.bn2.running_mean',\n",
       " 'layer1.2.bn2.running_var',\n",
       " 'layer1.2.bn2.num_batches_tracked',\n",
       " 'layer1.2.conv3.weight',\n",
       " 'layer1.2.bn3.weight',\n",
       " 'layer1.2.bn3.bias',\n",
       " 'layer1.2.bn3.running_mean',\n",
       " 'layer1.2.bn3.running_var',\n",
       " 'layer1.2.bn3.num_batches_tracked',\n",
       " 'layer2.0.conv1.weight',\n",
       " 'layer2.0.bn1.weight',\n",
       " 'layer2.0.bn1.bias',\n",
       " 'layer2.0.bn1.running_mean',\n",
       " 'layer2.0.bn1.running_var',\n",
       " 'layer2.0.bn1.num_batches_tracked',\n",
       " 'layer2.0.conv2.weight',\n",
       " 'layer2.0.bn2.weight',\n",
       " 'layer2.0.bn2.bias',\n",
       " 'layer2.0.bn2.running_mean',\n",
       " 'layer2.0.bn2.running_var',\n",
       " 'layer2.0.bn2.num_batches_tracked',\n",
       " 'layer2.0.conv3.weight',\n",
       " 'layer2.0.bn3.weight',\n",
       " 'layer2.0.bn3.bias',\n",
       " 'layer2.0.bn3.running_mean',\n",
       " 'layer2.0.bn3.running_var',\n",
       " 'layer2.0.bn3.num_batches_tracked',\n",
       " 'layer2.0.downsample.0.weight',\n",
       " 'layer2.0.downsample.1.weight',\n",
       " 'layer2.0.downsample.1.bias',\n",
       " 'layer2.0.downsample.1.running_mean',\n",
       " 'layer2.0.downsample.1.running_var',\n",
       " 'layer2.0.downsample.1.num_batches_tracked',\n",
       " 'layer2.1.conv1.weight',\n",
       " 'layer2.1.bn1.weight',\n",
       " 'layer2.1.bn1.bias',\n",
       " 'layer2.1.bn1.running_mean',\n",
       " 'layer2.1.bn1.running_var',\n",
       " 'layer2.1.bn1.num_batches_tracked',\n",
       " 'layer2.1.conv2.weight',\n",
       " 'layer2.1.bn2.weight',\n",
       " 'layer2.1.bn2.bias',\n",
       " 'layer2.1.bn2.running_mean',\n",
       " 'layer2.1.bn2.running_var',\n",
       " 'layer2.1.bn2.num_batches_tracked',\n",
       " 'layer2.1.conv3.weight',\n",
       " 'layer2.1.bn3.weight',\n",
       " 'layer2.1.bn3.bias',\n",
       " 'layer2.1.bn3.running_mean',\n",
       " 'layer2.1.bn3.running_var',\n",
       " 'layer2.1.bn3.num_batches_tracked',\n",
       " 'layer2.2.conv1.weight',\n",
       " 'layer2.2.bn1.weight',\n",
       " 'layer2.2.bn1.bias',\n",
       " 'layer2.2.bn1.running_mean',\n",
       " 'layer2.2.bn1.running_var',\n",
       " 'layer2.2.bn1.num_batches_tracked',\n",
       " 'layer2.2.conv2.weight',\n",
       " 'layer2.2.bn2.weight',\n",
       " 'layer2.2.bn2.bias',\n",
       " 'layer2.2.bn2.running_mean',\n",
       " 'layer2.2.bn2.running_var',\n",
       " 'layer2.2.bn2.num_batches_tracked',\n",
       " 'layer2.2.conv3.weight',\n",
       " 'layer2.2.bn3.weight',\n",
       " 'layer2.2.bn3.bias',\n",
       " 'layer2.2.bn3.running_mean',\n",
       " 'layer2.2.bn3.running_var',\n",
       " 'layer2.2.bn3.num_batches_tracked',\n",
       " 'layer2.3.conv1.weight',\n",
       " 'layer2.3.bn1.weight',\n",
       " 'layer2.3.bn1.bias',\n",
       " 'layer2.3.bn1.running_mean',\n",
       " 'layer2.3.bn1.running_var',\n",
       " 'layer2.3.bn1.num_batches_tracked',\n",
       " 'layer2.3.conv2.weight',\n",
       " 'layer2.3.bn2.weight',\n",
       " 'layer2.3.bn2.bias',\n",
       " 'layer2.3.bn2.running_mean',\n",
       " 'layer2.3.bn2.running_var',\n",
       " 'layer2.3.bn2.num_batches_tracked',\n",
       " 'layer2.3.conv3.weight',\n",
       " 'layer2.3.bn3.weight',\n",
       " 'layer2.3.bn3.bias',\n",
       " 'layer2.3.bn3.running_mean',\n",
       " 'layer2.3.bn3.running_var',\n",
       " 'layer2.3.bn3.num_batches_tracked',\n",
       " 'layer3.0.conv1.weight',\n",
       " 'layer3.0.bn1.weight',\n",
       " 'layer3.0.bn1.bias',\n",
       " 'layer3.0.bn1.running_mean',\n",
       " 'layer3.0.bn1.running_var',\n",
       " 'layer3.0.bn1.num_batches_tracked',\n",
       " 'layer3.0.conv2.weight',\n",
       " 'layer3.0.bn2.weight',\n",
       " 'layer3.0.bn2.bias',\n",
       " 'layer3.0.bn2.running_mean',\n",
       " 'layer3.0.bn2.running_var',\n",
       " 'layer3.0.bn2.num_batches_tracked',\n",
       " 'layer3.0.conv3.weight',\n",
       " 'layer3.0.bn3.weight',\n",
       " 'layer3.0.bn3.bias',\n",
       " 'layer3.0.bn3.running_mean',\n",
       " 'layer3.0.bn3.running_var',\n",
       " 'layer3.0.bn3.num_batches_tracked',\n",
       " 'layer3.0.downsample.0.weight',\n",
       " 'layer3.0.downsample.1.weight',\n",
       " 'layer3.0.downsample.1.bias',\n",
       " 'layer3.0.downsample.1.running_mean',\n",
       " 'layer3.0.downsample.1.running_var',\n",
       " 'layer3.0.downsample.1.num_batches_tracked',\n",
       " 'layer3.1.conv1.weight',\n",
       " 'layer3.1.bn1.weight',\n",
       " 'layer3.1.bn1.bias',\n",
       " 'layer3.1.bn1.running_mean',\n",
       " 'layer3.1.bn1.running_var',\n",
       " 'layer3.1.bn1.num_batches_tracked',\n",
       " 'layer3.1.conv2.weight',\n",
       " 'layer3.1.bn2.weight',\n",
       " 'layer3.1.bn2.bias',\n",
       " 'layer3.1.bn2.running_mean',\n",
       " 'layer3.1.bn2.running_var',\n",
       " 'layer3.1.bn2.num_batches_tracked',\n",
       " 'layer3.1.conv3.weight',\n",
       " 'layer3.1.bn3.weight',\n",
       " 'layer3.1.bn3.bias',\n",
       " 'layer3.1.bn3.running_mean',\n",
       " 'layer3.1.bn3.running_var',\n",
       " 'layer3.1.bn3.num_batches_tracked',\n",
       " 'layer3.2.conv1.weight',\n",
       " 'layer3.2.bn1.weight',\n",
       " 'layer3.2.bn1.bias',\n",
       " 'layer3.2.bn1.running_mean',\n",
       " 'layer3.2.bn1.running_var',\n",
       " 'layer3.2.bn1.num_batches_tracked',\n",
       " 'layer3.2.conv2.weight',\n",
       " 'layer3.2.bn2.weight',\n",
       " 'layer3.2.bn2.bias',\n",
       " 'layer3.2.bn2.running_mean',\n",
       " 'layer3.2.bn2.running_var',\n",
       " 'layer3.2.bn2.num_batches_tracked',\n",
       " 'layer3.2.conv3.weight',\n",
       " 'layer3.2.bn3.weight',\n",
       " 'layer3.2.bn3.bias',\n",
       " 'layer3.2.bn3.running_mean',\n",
       " 'layer3.2.bn3.running_var',\n",
       " 'layer3.2.bn3.num_batches_tracked',\n",
       " 'layer3.3.conv1.weight',\n",
       " 'layer3.3.bn1.weight',\n",
       " 'layer3.3.bn1.bias',\n",
       " 'layer3.3.bn1.running_mean',\n",
       " 'layer3.3.bn1.running_var',\n",
       " 'layer3.3.bn1.num_batches_tracked',\n",
       " 'layer3.3.conv2.weight',\n",
       " 'layer3.3.bn2.weight',\n",
       " 'layer3.3.bn2.bias',\n",
       " 'layer3.3.bn2.running_mean',\n",
       " 'layer3.3.bn2.running_var',\n",
       " 'layer3.3.bn2.num_batches_tracked',\n",
       " 'layer3.3.conv3.weight',\n",
       " 'layer3.3.bn3.weight',\n",
       " 'layer3.3.bn3.bias',\n",
       " 'layer3.3.bn3.running_mean',\n",
       " 'layer3.3.bn3.running_var',\n",
       " 'layer3.3.bn3.num_batches_tracked',\n",
       " 'layer3.4.conv1.weight',\n",
       " 'layer3.4.bn1.weight',\n",
       " 'layer3.4.bn1.bias',\n",
       " 'layer3.4.bn1.running_mean',\n",
       " 'layer3.4.bn1.running_var',\n",
       " 'layer3.4.bn1.num_batches_tracked',\n",
       " 'layer3.4.conv2.weight',\n",
       " 'layer3.4.bn2.weight',\n",
       " 'layer3.4.bn2.bias',\n",
       " 'layer3.4.bn2.running_mean',\n",
       " 'layer3.4.bn2.running_var',\n",
       " 'layer3.4.bn2.num_batches_tracked',\n",
       " 'layer3.4.conv3.weight',\n",
       " 'layer3.4.bn3.weight',\n",
       " 'layer3.4.bn3.bias',\n",
       " 'layer3.4.bn3.running_mean',\n",
       " 'layer3.4.bn3.running_var',\n",
       " 'layer3.4.bn3.num_batches_tracked',\n",
       " 'layer3.5.conv1.weight',\n",
       " 'layer3.5.bn1.weight',\n",
       " 'layer3.5.bn1.bias',\n",
       " 'layer3.5.bn1.running_mean',\n",
       " 'layer3.5.bn1.running_var',\n",
       " 'layer3.5.bn1.num_batches_tracked',\n",
       " 'layer3.5.conv2.weight',\n",
       " 'layer3.5.bn2.weight',\n",
       " 'layer3.5.bn2.bias',\n",
       " 'layer3.5.bn2.running_mean',\n",
       " 'layer3.5.bn2.running_var',\n",
       " 'layer3.5.bn2.num_batches_tracked',\n",
       " 'layer3.5.conv3.weight',\n",
       " 'layer3.5.bn3.weight',\n",
       " 'layer3.5.bn3.bias',\n",
       " 'layer3.5.bn3.running_mean',\n",
       " 'layer3.5.bn3.running_var',\n",
       " 'layer3.5.bn3.num_batches_tracked',\n",
       " 'layer4.0.conv1.weight',\n",
       " 'layer4.0.bn1.weight',\n",
       " 'layer4.0.bn1.bias',\n",
       " 'layer4.0.bn1.running_mean',\n",
       " 'layer4.0.bn1.running_var',\n",
       " 'layer4.0.bn1.num_batches_tracked',\n",
       " 'layer4.0.conv2.weight',\n",
       " 'layer4.0.bn2.weight',\n",
       " 'layer4.0.bn2.bias',\n",
       " 'layer4.0.bn2.running_mean',\n",
       " 'layer4.0.bn2.running_var',\n",
       " 'layer4.0.bn2.num_batches_tracked',\n",
       " 'layer4.0.conv3.weight',\n",
       " 'layer4.0.bn3.weight',\n",
       " 'layer4.0.bn3.bias',\n",
       " 'layer4.0.bn3.running_mean',\n",
       " 'layer4.0.bn3.running_var',\n",
       " 'layer4.0.bn3.num_batches_tracked',\n",
       " 'layer4.0.downsample.0.weight',\n",
       " 'layer4.0.downsample.1.weight',\n",
       " 'layer4.0.downsample.1.bias',\n",
       " 'layer4.0.downsample.1.running_mean',\n",
       " 'layer4.0.downsample.1.running_var',\n",
       " 'layer4.0.downsample.1.num_batches_tracked',\n",
       " 'layer4.1.conv1.weight',\n",
       " 'layer4.1.bn1.weight',\n",
       " 'layer4.1.bn1.bias',\n",
       " 'layer4.1.bn1.running_mean',\n",
       " 'layer4.1.bn1.running_var',\n",
       " 'layer4.1.bn1.num_batches_tracked',\n",
       " 'layer4.1.conv2.weight',\n",
       " 'layer4.1.bn2.weight',\n",
       " 'layer4.1.bn2.bias',\n",
       " 'layer4.1.bn2.running_mean',\n",
       " 'layer4.1.bn2.running_var',\n",
       " 'layer4.1.bn2.num_batches_tracked',\n",
       " 'layer4.1.conv3.weight',\n",
       " 'layer4.1.bn3.weight',\n",
       " 'layer4.1.bn3.bias',\n",
       " 'layer4.1.bn3.running_mean',\n",
       " 'layer4.1.bn3.running_var',\n",
       " 'layer4.1.bn3.num_batches_tracked',\n",
       " 'layer4.2.conv1.weight',\n",
       " 'layer4.2.bn1.weight',\n",
       " 'layer4.2.bn1.bias',\n",
       " 'layer4.2.bn1.running_mean',\n",
       " 'layer4.2.bn1.running_var',\n",
       " 'layer4.2.bn1.num_batches_tracked',\n",
       " 'layer4.2.conv2.weight',\n",
       " 'layer4.2.bn2.weight',\n",
       " 'layer4.2.bn2.bias',\n",
       " 'layer4.2.bn2.running_mean',\n",
       " 'layer4.2.bn2.running_var',\n",
       " 'layer4.2.bn2.num_batches_tracked',\n",
       " 'layer4.2.conv3.weight',\n",
       " 'layer4.2.bn3.weight',\n",
       " 'layer4.2.bn3.bias',\n",
       " 'layer4.2.bn3.running_mean',\n",
       " 'layer4.2.bn3.running_var',\n",
       " 'layer4.2.bn3.num_batches_tracked',\n",
       " 'fc.weight',\n",
       " 'fc.bias']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[k for k in state_dict.keys()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[0.5093, 0.5100, 0.4907, 0.4860, 0.4746, 0.4731, 0.4552],\n",
      "         [0.5036, 0.5046, 0.5103, 0.5141, 0.5025, 0.4874, 0.4760],\n",
      "         [0.5156, 0.5163, 0.5114, 0.5390, 0.5673, 0.5415, 0.5345],\n",
      "         [0.5121, 0.4991, 0.4672, 0.4664, 0.4879, 0.5220, 0.5384],\n",
      "         [0.4980, 0.5144, 0.4903, 0.4630, 0.4177, 0.4471, 0.5012],\n",
      "         [0.4983, 0.5267, 0.5357, 0.5502, 0.5118, 0.4744, 0.4858],\n",
      "         [0.4449, 0.4754, 0.4847, 0.5182, 0.5191, 0.5105, 0.4972]],\n",
      "\n",
      "        [[0.4885, 0.5075, 0.5155, 0.5357, 0.5349, 0.5285, 0.4941],\n",
      "         [0.4954, 0.5123, 0.5437, 0.5840, 0.6027, 0.5937, 0.5769],\n",
      "         [0.4711, 0.4517, 0.4430, 0.4985, 0.5783, 0.6074, 0.6132],\n",
      "         [0.4745, 0.4185, 0.3405, 0.2969, 0.3387, 0.4462, 0.5209],\n",
      "         [0.5185, 0.5079, 0.4457, 0.3451, 0.2566, 0.3000, 0.4047],\n",
      "         [0.5487, 0.5844, 0.5932, 0.5789, 0.4927, 0.4160, 0.4125],\n",
      "         [0.4912, 0.5451, 0.5855, 0.6324, 0.6143, 0.5672, 0.5108]],\n",
      "\n",
      "        [[0.4888, 0.4968, 0.5060, 0.5214, 0.5172, 0.5174, 0.4979],\n",
      "         [0.4941, 0.5031, 0.5323, 0.5691, 0.5805, 0.5771, 0.5721],\n",
      "         [0.4601, 0.4357, 0.4377, 0.4983, 0.5691, 0.5904, 0.5964],\n",
      "         [0.4823, 0.4308, 0.3606, 0.3353, 0.3734, 0.4636, 0.5254],\n",
      "         [0.5172, 0.5071, 0.4527, 0.3766, 0.3001, 0.3367, 0.4214],\n",
      "         [0.5425, 0.5673, 0.5796, 0.5718, 0.4922, 0.4154, 0.4159],\n",
      "         [0.4920, 0.5386, 0.5742, 0.6325, 0.6191, 0.5693, 0.5110]]])\n",
      "tensor([[[0.5093, 0.4885, 0.4888],\n",
      "         [0.5100, 0.5075, 0.4968],\n",
      "         [0.4907, 0.5155, 0.5060],\n",
      "         [0.4860, 0.5357, 0.5214],\n",
      "         [0.4746, 0.5349, 0.5172],\n",
      "         [0.4731, 0.5285, 0.5174],\n",
      "         [0.4552, 0.4941, 0.4979]],\n",
      "\n",
      "        [[0.5036, 0.4954, 0.4941],\n",
      "         [0.5046, 0.5123, 0.5031],\n",
      "         [0.5103, 0.5437, 0.5323],\n",
      "         [0.5141, 0.5840, 0.5691],\n",
      "         [0.5025, 0.6027, 0.5805],\n",
      "         [0.4874, 0.5937, 0.5771],\n",
      "         [0.4760, 0.5769, 0.5721]],\n",
      "\n",
      "        [[0.5156, 0.4711, 0.4601],\n",
      "         [0.5163, 0.4517, 0.4357],\n",
      "         [0.5114, 0.4430, 0.4377],\n",
      "         [0.5390, 0.4985, 0.4983],\n",
      "         [0.5673, 0.5783, 0.5691],\n",
      "         [0.5415, 0.6074, 0.5904],\n",
      "         [0.5345, 0.6132, 0.5964]],\n",
      "\n",
      "        [[0.5121, 0.4745, 0.4823],\n",
      "         [0.4991, 0.4185, 0.4308],\n",
      "         [0.4672, 0.3405, 0.3606],\n",
      "         [0.4664, 0.2969, 0.3353],\n",
      "         [0.4879, 0.3387, 0.3734],\n",
      "         [0.5220, 0.4462, 0.4636],\n",
      "         [0.5384, 0.5209, 0.5254]],\n",
      "\n",
      "        [[0.4980, 0.5185, 0.5172],\n",
      "         [0.5144, 0.5079, 0.5071],\n",
      "         [0.4903, 0.4457, 0.4527],\n",
      "         [0.4630, 0.3451, 0.3766],\n",
      "         [0.4177, 0.2566, 0.3001],\n",
      "         [0.4471, 0.3000, 0.3367],\n",
      "         [0.5012, 0.4047, 0.4214]],\n",
      "\n",
      "        [[0.4983, 0.5487, 0.5425],\n",
      "         [0.5267, 0.5844, 0.5673],\n",
      "         [0.5357, 0.5932, 0.5796],\n",
      "         [0.5502, 0.5789, 0.5718],\n",
      "         [0.5118, 0.4927, 0.4922],\n",
      "         [0.4744, 0.4160, 0.4154],\n",
      "         [0.4858, 0.4125, 0.4159]],\n",
      "\n",
      "        [[0.4449, 0.4912, 0.4920],\n",
      "         [0.4754, 0.5451, 0.5386],\n",
      "         [0.4847, 0.5855, 0.5742],\n",
      "         [0.5182, 0.6324, 0.6325],\n",
      "         [0.5191, 0.6143, 0.6191],\n",
      "         [0.5105, 0.5672, 0.5693],\n",
      "         [0.4972, 0.5108, 0.5110]]])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x1700 with 64 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Ensure it's in evaluation mode for visualization\n",
    "model_res50.eval()  # This changes the model to evaluation mode\n",
    "\n",
    "# Get the weights of the first convolutional layer\n",
    "weights = model_res50.conv1.weight.data.cpu()\n",
    "\n",
    "# Normalize the weights for better visualization\n",
    "weights = (weights - weights.min()) / (weights.max() - weights.min())\n",
    "print(weights[0])\n",
    "print(weights[0].permute(1, 2, 0))\n",
    "# Plot the first few filters\n",
    "plt.figure(figsize=(20, 17))\n",
    "for i in range(64):  # Let's visualize 64 filters\n",
    "    plt.subplot(8, 8, i + 1)\n",
    "    plt.imshow(weights[i].permute(1, 2, 0))  # Rearrange the dimensions\n",
    "    plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ResNet(\n",
      "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (relu): ReLU(inplace=True)\n",
      "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  (layer1): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (3): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (4): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (5): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Bottleneck(\n",
      "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "      (downsample): Sequential(\n",
      "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "    (2): Bottleneck(\n",
      "      (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
      "  (fc): Linear(in_features=2048, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model_res50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ytorch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
